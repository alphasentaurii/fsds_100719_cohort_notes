{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sect 46: Tuning Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Study Group: 02-12-20\n",
    "- online-ds-ft-100719"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Announcements/Choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **When/if to cover PySpark sections (sect 40 & 41)?**\n",
    "    - Capstone starts next week.\n",
    "    - Two Sections require PySpark/Docker.\n",
    "        - (Honestly it just does enough to check the box of having covered it, but that's it.)\n",
    "- **One alternative option: give you the Deep Learning Project from curric v1.1** and talk through how to tackle it\n",
    "- **Section 50: Amazon Web Services**\n",
    "    - Let's save it for a couple weeks into capstone and use it in order to set up storage for web-dashboards deployed with dash/plotly and herokuapp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Revisit neural networks powerpoint with updated images to review biological inspiration of ANNs**\n",
    "- **Discuss details about deep neural networks:**\n",
    "    - what makes an ANN \"deep\"?\n",
    "    - what are the different activation functions?\n",
    "    - how do we do hyperparameter tuning/grid-searching?\n",
    "- **Discuss/demo using Keras neural network for image classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "- https://dashee87.github.io/deep%20learning/visualising-activation-functions-in-neural-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T16:57:56.234291Z",
     "start_time": "2020-02-12T16:57:56.210200Z"
    },
    "code_folding": [
     4,
     10,
     16,
     32,
     48,
     53
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def sigmoid(x, derivative=False):\n",
    "    f = 1 / (1 + np.exp(-x))\n",
    "    if (derivative == True):\n",
    "        return f * (1 - f)\n",
    "    return f\n",
    "\n",
    "def tanh(x, derivative=False):\n",
    "    f = np.tanh(x)\n",
    "    if (derivative == True):\n",
    "        return (1 - (f ** 2))\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x, derivative=False):\n",
    "    f = np.zeros(len(x))\n",
    "    if (derivative == True):\n",
    "        for i in range(0, len(x)):\n",
    "            if x[i] > 0:\n",
    "                f[i] = 1  \n",
    "            else:\n",
    "                f[i] = 0\n",
    "        return f\n",
    "    for i in range(0, len(x)):\n",
    "        if x[i] > 0:\n",
    "            f[i] = x[i]  \n",
    "        else:\n",
    "            f[i] = 0\n",
    "    return f\n",
    "\n",
    "def leaky_relu(x, leakage = 0.05, derivative=False):\n",
    "    f = np.zeros(len(x))\n",
    "    if (derivative == True):\n",
    "        for i in range(0, len(x)):\n",
    "            if x[i] > 0:\n",
    "                f[i] = 1  \n",
    "            else:\n",
    "                f[i] = leakage\n",
    "        return f\n",
    "    for i in range(0, len(x)):\n",
    "        if x[i] > 0:\n",
    "            f[i] = x[i]  \n",
    "        else:\n",
    "            f[i] = x[i]* leakage\n",
    "    return f\n",
    "\n",
    "def arctan(x, derivative=False):\n",
    "    if (derivative == True):\n",
    "        return 1/(1+np.square(x))\n",
    "    return np.arctan(x)\n",
    "\n",
    "def plot_activation(fn):\n",
    "    z = np.arange(-10, 10, 0.2)\n",
    "    y = fn(z)\n",
    "    dy = fn(z, derivative=True)\n",
    "    fig,ax=plt.subplots(figsize=(6,4))\n",
    "    ax.set_title(f'{fn.__name__}')\n",
    "    ax.set(xlabel='Input',ylabel='Output')\n",
    "    ax.axhline(color='gray', linewidth=1,)\n",
    "    ax.axvline(color='gray', linewidth=1,)\n",
    "    ax.plot(z, y, 'r', label='original (y)')\n",
    "    ax.plot(z, dy, 'b', label='derivative (dy)')\n",
    "    ax.legend();\n",
    "    plt.show()\n",
    "\n",
    "## Plot activation functions\n",
    "# act_funcs = [sigmoid,tanh,arctan,relu,leaky_relu]\n",
    "# [plot_activation(fn) for fn in act_funcs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification with MLPs - Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T16:57:56.239427Z",
     "start_time": "2020-02-12T16:57:56.235919Z"
    }
   },
   "outputs": [],
   "source": [
    "from fsds_100719.imports import *\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from keras import models, layers,optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense#,Dropout\n",
    "\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T16:57:56.534961Z",
     "start_time": "2020-02-12T16:57:56.241322Z"
    }
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(f'X_train.shape:\\t{X_train.shape}')\n",
    "print(f'X_test.shape:\\t{X_test.shape}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T16:57:56.737581Z",
     "start_time": "2020-02-12T16:57:56.536356Z"
    }
   },
   "outputs": [],
   "source": [
    "## Visualize random image\n",
    "i = np.random.choice(list(range(len(X_train))))\n",
    "sample_image = X_train[i]\n",
    "sample_label = y_train[i]\n",
    "\n",
    "title = f\"Image #{i}:  Label={sample_label}\"\n",
    "plt.imshow(sample_image, cmap='gray')\n",
    "\n",
    "ax= plt.gca()\n",
    "ax.set(title=title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T16:57:56.743166Z",
     "start_time": "2020-02-12T16:57:56.739444Z"
    }
   },
   "outputs": [],
   "source": [
    "## Print out image shapes and data shapes\n",
    "print(sample_image.shape)\n",
    "print(f\"X_train.shape={X_train.shape}\")\n",
    "print(f\"X_test.shape={X_test.shape}\")\n",
    "print(sample_image.shape[0]*sample_image.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T16:57:56.749532Z",
     "start_time": "2020-02-12T16:57:56.744657Z"
    }
   },
   "outputs": [],
   "source": [
    "## SAVE SHAPES FOR EASY ACCESS LATE\n",
    "X_shapes = dict(X_train=X_train.shape,\n",
    "                X_test=X_test.shape,\n",
    "               y_train=y_train.shape,\n",
    "               y_test=y_test.shape,\n",
    "               image=sample_image.shape,\n",
    "                image_flat = sample_image.shape[0]*sample_image.shape[1]\n",
    "               )\n",
    "X_shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***We can interpret these numbers as saying \"X_train consists of 60,000 images that are 28x28\". We'll need to reshape them from (28, 28), a 28x28 matrix, to (784,), a 784-element vector. However, we need to make sure that the first number in our reshape call for both X_train and X_test still correspond to the number of observations we have in each.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T16:57:56.755274Z",
     "start_time": "2020-02-12T16:57:56.751064Z"
    }
   },
   "outputs": [],
   "source": [
    "shapes = list(sample_image.shape)\n",
    "shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T16:57:57.051008Z",
     "start_time": "2020-02-12T16:57:56.757443Z"
    }
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_shapes = dict(X_train=X_train.shape,\n",
    "               X_test=X_test.shape,\n",
    "               y_train=y_train.shape,\n",
    "                y_test=y_test.shape,\n",
    "               image=sample_image.shape,\n",
    "               image_unrow = sample_image.shape[0]*sample_image.shape[1],)\n",
    "\n",
    "print(X_shapes['X_train'])\n",
    "print(X_shapes['image_unrow'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Targets\n",
    "\n",
    "- This is a **Multiclass Classification** problem.\n",
    "    - we need to One-Hot Encode our labels\n",
    "    - `keras.utils.to_categorical`\n",
    "    \n",
    "- For multi classification:\n",
    "    - good final activation function is softmax\n",
    "    - categorical_crossentytropu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T16:57:57.220108Z",
     "start_time": "2020-02-12T16:57:57.052369Z"
    }
   },
   "outputs": [],
   "source": [
    "#  reshape and convert astype('float32') so that we convert our data from type uint8 to float32\n",
    "X_train = X_train.reshape(X_shapes['X_train'][0], X_shapes['image_unrow']).astype('float32')\n",
    "X_test = X_test.reshape(X_shapes['X_test'][0], X_shapes['image_unrow']).astype('float32')\n",
    "\n",
    "## normalizing data \n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "y_train_orig = y_train.copy()\n",
    "y_test_orig = y_test.copy()\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "print(f\"y_train original shape = {y_train_orig.shape}\")\n",
    "print(f\"y_train to_categorical shape = {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T16:57:57.226773Z",
     "start_time": "2020-02-12T16:57:57.221382Z"
    }
   },
   "outputs": [],
   "source": [
    "## Taking argmax(axis=1) of to_categorical converts it back\n",
    "all(y_train.argmax(axis=1) == y_train_orig) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T17:05:28.479490Z",
     "start_time": "2020-02-12T17:05:28.473134Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(y_true,y_pred=None,X=None,model=None,history=None):\n",
    "    \"\"\"Uses plot_keras_history and plot_confusion_matrix\n",
    "    to display result of keras classification.\n",
    "    Also prints the classification report.\"\"\"\n",
    "    if (y_pred is None):\n",
    "        if (X is None):\n",
    "            raise Exception('You must either provide X data + model OR y_pred.')\n",
    "        else:\n",
    "            if model is not None:\n",
    "                y_pred = model.predict_classes(X)\n",
    "            else:\n",
    "                raise Exception('If using X data you must also provide model.')\n",
    "            \n",
    "    if history is not None:\n",
    "        plot_keras_history(history)\n",
    "\n",
    "\n",
    "    ## reshape keras categorical for sklearn\n",
    "    if y_true.ndim>1:\n",
    "        y_true = y_true.argmax(axis=1)\n",
    "    if y_pred.ndim>1:\n",
    "        y_pred = y_pred.argmax(axis=1)\n",
    "\n",
    "        \n",
    "    import sklearn.metrics as metrics\n",
    "    class_report = metrics.classification_report(y_true,y_pred)\n",
    "    print(class_report)\n",
    "    \n",
    "    try:\n",
    "        fig = plot_confusion_matrix((y_true,y_pred),normalize=True)\n",
    "        display(fig)\n",
    "    except Exception as e:\n",
    "        print('Error with confusion matrix.:')\n",
    "        print(f'\\t{e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T17:05:30.355060Z",
     "start_time": "2020-02-12T17:05:30.338252Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_keras_history(history,figsize=(10,4),subplot_kws={}):\n",
    "    if hasattr(history,'history'):\n",
    "        history=history.history\n",
    "    #     history = results.history\n",
    "    fig,axes=plt.subplots(ncols=2,figsize=figsize,**subplot_kws)\n",
    "    \n",
    "    ax=axes[0]\n",
    "    ax.plot(history['val_loss'],label='val_loss')\n",
    "    ax.plot(history['loss'], label='loss')\n",
    "    ax.legend()\n",
    "    \n",
    "    ax=axes[1]\n",
    "    ax.plot(history['val_accuracy'],label='val_accuracy')\n",
    "    ax.plot(history['accuracy'], label='accuracy')\n",
    "\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def plot_confusion_matrix(conf_matrix, classes = None, normalize=True,\n",
    "                          title='Confusion Matrix', cmap=\"Blues\",\n",
    "                          print_raw_matrix=False,\n",
    "                          fig_size=(7,8)):\n",
    "    \"\"\"Check if Normalization Option is Set to True. \n",
    "    If so, normalize the raw confusion matrix before visualizing\n",
    "    #Other code should be equivalent to your previous function.\n",
    "    Note: Taken from bs_ds and modified\n",
    "    - Can pass a tuple of (y_true,y_pred) instead of conf matrix.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    ## make confusion matrix if given tuple of y_true,y_pred\n",
    "    if isinstance(conf_matrix, tuple):\n",
    "        y_true = conf_matrix[0].copy()\n",
    "        y_pred = conf_matrix[1].copy()\n",
    "        \n",
    "        ## collapse down to 1 dimension again\n",
    "        if y_true.ndim>1:\n",
    "            y_true = y_true.argmax(axis=1)\n",
    "        if y_pred.ndim>1:\n",
    "            y_pred = y_pred.argmax(axis=1)\n",
    "        cm = metrics.confusion_matrix(y_true,y_pred)\n",
    "        \n",
    "    else:\n",
    "        cm = conf_matrix\n",
    "        \n",
    "    ## Generate integer labels for classes\n",
    "    if classes is None:\n",
    "        classes = list(range(len(cm)))  \n",
    "        \n",
    "\n",
    "    ## Text Properties\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "\n",
    "    ## Normalize data\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "        \n",
    "    fontDict = {\n",
    "        'title':{\n",
    "            'fontsize':16,\n",
    "            'fontweight':'semibold',\n",
    "            'ha':'center',\n",
    "            },\n",
    "        'xlabel':{\n",
    "            'fontsize':14,\n",
    "            'fontweight':'normal',\n",
    "            },\n",
    "        'ylabel':{\n",
    "            'fontsize':14,\n",
    "            'fontweight':'normal',\n",
    "            },\n",
    "        'xtick_labels':{\n",
    "            'fontsize':10,\n",
    "            'fontweight':'normal',\n",
    "#             'rotation':45,\n",
    "            'ha':'right',\n",
    "            },\n",
    "        'ytick_labels':{\n",
    "            'fontsize':10,\n",
    "            'fontweight':'normal',\n",
    "            'rotation':0,\n",
    "            'ha':'right',\n",
    "            },\n",
    "        'data_labels':{\n",
    "            'ha':'center',\n",
    "            'fontweight':'semibold',\n",
    "\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    # Create plot\n",
    "    fig,ax = plt.subplots(figsize=fig_size)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title,**fontDict['title'])\n",
    "    plt.colorbar()\n",
    "\n",
    "    tick_marks = classes#np.arange(len(classes))\n",
    "\n",
    "\n",
    "    plt.xticks(tick_marks, classes, **fontDict['xtick_labels'])\n",
    "    plt.yticks(tick_marks, classes,**fontDict['ytick_labels'])\n",
    "\n",
    "    # Determine threshold for b/w text\n",
    "    thresh = cm.max() / 2.\n",
    "\n",
    "    # fig,ax = plt.subplots()\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 color='darkgray',**fontDict['data_labels']) #color=\"white\" if cm[i, j] > thresh else \"black\"\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label',**fontDict['ylabel'])\n",
    "    plt.xlabel('Predicted label',**fontDict['xlabel'])\n",
    "\n",
    "    if print_raw_matrix:\n",
    "        print_title = 'Raw Confusion Matrix Counts:'\n",
    "        print('\\n',print_title)\n",
    "        print(conf_matrix)\n",
    "\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T17:05:32.081953Z",
     "start_time": "2020-02-12T17:05:31.983829Z"
    }
   },
   "outputs": [],
   "source": [
    "## build network\n",
    "\n",
    "## Model 3\n",
    "model3= models.Sequential()\n",
    "model3.add( \n",
    "    Dense(64, activation='relu', input_shape=(X_shapes['image_unrow'],) ))\n",
    "model3.add(Dense(32, activation='relu'))\n",
    "model3.add(Dense(10, activation='softmax'))\n",
    "\n",
    "compile_kws=dict(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "model3.compile(**compile_kws)\n",
    "\n",
    "model3.summary()\n",
    "# model.summary()\n",
    "\n",
    "model=model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T17:05:41.060250Z",
     "start_time": "2020-02-12T17:05:32.214601Z"
    }
   },
   "outputs": [],
   "source": [
    "## Train network\n",
    "history = model.fit(X_train, y_train, batch_size=64, epochs= 5,\n",
    "                   validation_data=(X_test,y_test))\n",
    "evaluate_model(y_test,X=X_test,model=model,history=history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T17:05:41.063868Z",
     "start_time": "2020-02-12T17:05:41.061799Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot_confusion_matrix((y_test,y_hat_test));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T17:05:41.205647Z",
     "start_time": "2020-02-12T17:05:41.065719Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "y_hat_test = model.predict_classes(X_test)\n",
    "print(y_hat_test.shape)\n",
    "print(np.array(y_hat_test).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T17:05:41.222206Z",
     "start_time": "2020-02-12T17:05:41.207110Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Must convert \n",
    "metrics.confusion_matrix(y_test.argmax(axis=1),y_hat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T16:24:57.597320Z",
     "start_time": "2020-02-12T16:24:57.592601Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperParameter Tuning with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original Source: https://chrisalbon.com/deep_learning/keras/tuning_neural_network_hyperparameters/\n",
    "<br><br>\n",
    "\n",
    "- To use `GridSearchCV` or other similar functions in scikit-learn with a Keras neural network, we need to wrap our keras model in `keras.wrappers.scikit_learn`'s `KerasClassifier` and `KerasRegressor`.\n",
    "1. To do this, we need to write a build function(`build_fn`) that creates our model such as `create_model`.\n",
    "    - This function must accept whatever parameters you wish to tune. \n",
    "    - It also must have a default argument for each parameter.\n",
    "    - This function must Return the model (and only the model)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "## Define the build function\n",
    "def create_model(n_units=(50,25,7), activation='relu',final_activation='softmax',\n",
    "                optimizer='adam'):\n",
    "    \n",
    "    ## Pro tip:save the local variables now so you can print out the parameters used to create the model.\n",
    "    params_used = locals()\n",
    "    print('Parameters for model:\\n',params_used)\n",
    "    \n",
    "   \n",
    "    from keras.models import Sequential\n",
    "    from keras import layers\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(layers.Dense(n_units[0], activation=activation, input_shape=(2000,)))\n",
    "    model.add(layers.Dense(n_units[1], activation=activation))\n",
    "    model.add(layers.Dense(n_units[2], activation=final_activation))\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    display(model.summary())\n",
    "    return model \n",
    "```    \n",
    "\n",
    "2. We then create out model using the Keras wrapper:\n",
    "\n",
    "```python\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "neural_network =  KerasClassifier(build_fn=create_model,verbose=1)\n",
    "```\n",
    "\n",
    "3. Now, set up the hyperparameter space for grid search. (Remember, your `create_model` function must accept the parameter you want to tune)\n",
    "\n",
    "```python\n",
    "params_to_test = {'n_units':[(50,25,7),(100,50,7)],\n",
    "                  'optimizer':['adam','rmsprop','adadelta'],\n",
    "                  'activation':['linear','relu','tanh'],\n",
    "                  'final_activation':['softmax']}\n",
    "```\n",
    "\n",
    "4. Now instantiate your GridSearch function\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid = GridSearchCV(estimator=neural_network,param_grid=params_to_test)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "best_params = grid_result.best_params_\n",
    "```\n",
    "5. And thats it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T17:10:24.035882Z",
     "start_time": "2020-02-12T17:10:24.024562Z"
    }
   },
   "outputs": [],
   "source": [
    "class Timer():\n",
    "    def __init__(self, start=True,time_fmt='%m/%d/%y - %T'):\n",
    "        import tzlocal\n",
    "        import datetime as dt\n",
    "        \n",
    "        self.tz = tzlocal.get_localzone()\n",
    "        self.fmt= time_fmt\n",
    "        self._created = dt.datetime.now(tz=self.tz)\n",
    "        \n",
    "        if start:\n",
    "            self.start()\n",
    "            \n",
    "    def get_time(self):\n",
    "        import datetime as dt\n",
    "        return dt.datetime.now(tz=self.tz)\n",
    "\n",
    "        \n",
    "    def start(self,verbose=True):\n",
    "        self._laps_completed = 0\n",
    "        self.start = self.get_time()\n",
    "        if verbose: \n",
    "            print(f'[i] Timer started at {self.start.strftime(self.fmt)}')\n",
    "    \n",
    "    def stop(self, verbose=True):\n",
    "        self._laps_completed += 1\n",
    "        self.end = self.get_time()\n",
    "        self.elapsed = self.end -  self.start\n",
    "        if verbose: \n",
    "            print(f'[i] Timer stopped at {self.end.strftime(self.fmt)}')\n",
    "            print(f'  - Total Time: {self.elapsed}')\n",
    "            \n",
    "timer = Timer()\n",
    "import time\n",
    "timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T17:05:41.233947Z",
     "start_time": "2020-02-12T17:05:41.228837Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(optimizer='sgd' ,verbose=True):\n",
    "\n",
    "    model3= models.Sequential()\n",
    "    model3.add( \n",
    "        Dense(64, activation='relu', input_shape=(X_shapes['image_unrow'],) ))\n",
    "    model3.add(Dense(32, activation='relu'))\n",
    "    model3.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    compile_kws=dict()\n",
    "    model3.compile(loss='categorical_crossentropy',metrics=['accuracy'], optimizer=optimizer)\n",
    "    if verbose:\n",
    "        model3.summary()\n",
    "        \n",
    "    return model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T17:52:54.171692Z",
     "start_time": "2020-02-12T17:52:54.159316Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'keras.layers.advanced_activations' from '//anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/layers/advanced_activations.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras import layers, models,optimizers\n",
    "# neural_network =  KerasClassifier(build_fn=build_model,verbose=1)\n",
    "leaky_relu = layers.advanced_activations.LeakyReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T17:11:13.437298Z",
     "start_time": "2020-02-12T17:10:38.980205Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classifier = KerasClassifier(build_fn = build_model)\n",
    "parameters = {'batch_size': [25, 32],\n",
    "              'epochs': [10, 20],\n",
    "              'optimizer': ['adam', 'rmsprop']}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = classifier,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 10)\n",
    "\n",
    "\n",
    "timer = Timer()\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "best_parameters = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_\n",
    "timer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Ruh Roh! It doesn't seem to want to work with our data. Thats due to the scoring functions in sklearn not accepting y_data with ndim >1***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Why would you do this?**\n",
    "    1. You may want to use a metric that isn't available in sklearn. \n",
    "        - In the included `my_custom_scorer` function, I take the accuracy of each class's predictions from the diagonal of a normalized confusion matrix. \n",
    "        - I then calculate the mean of those 3 class accuracies, which is the `score` that is returned to the gridsearch. \n",
    "        \n",
    "    2. You may want to add a printout or display to the scoring function so you can see the results as the search is going.\n",
    "<br><br>\n",
    "2. **How do you do write your own?**\n",
    "    1. Define your custom scoring function.\n",
    "        - It must accept `y_true`,`y_pred`\n",
    "        - It must return a value to maximize. (like accuracy)\n",
    "    2. You can add print or display commands to have the scoring function report the current results as the gridsearch is still going.\n",
    "        - If you combine this with the example `create_model` function above that includes the `vars=locals(); print(vars)` command, then gridsearch will display:\n",
    "            1. the parameters of each model (each time the `create_model` function is called.\n",
    "            2. The score of each model, including a confusion matrix figure (each time it calls `my_custom_scorer`).\n",
    "        \n",
    "```python\n",
    "def my_custom_scorer(y_true,y_pred):\n",
    "    \"\"\"My custom score function to use with sklearn's GridSearchCV\n",
    "    Maximizes the average accuracy per class using a normalized confusion matrix\"\"\"\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import numpy as np\n",
    "    import functions_combined_BEST as ji    \n",
    "\n",
    "    ## Flatten one-hot encoded target columns into 1 column for sklearn functions\n",
    "    if y_true.ndim>1 or y_pred.ndim>1:\n",
    "        \n",
    "        ## reduce dimensions of y_train and y_test\n",
    "        if y_true.ndim>1:            \n",
    "            y_true = y_true.argmax(axis=1)\n",
    "        \n",
    "        if y_pred.ndim>1:\n",
    "            y_pred = y_pred.argmax(axis=1)\n",
    "\n",
    "    \n",
    "     # Get confusion matrx\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Normalize confusion matrix\n",
    "    cm_norm = cm.astype('float')/cm.sum(axis=1)[:,np.newaxis]\n",
    "\n",
    "    ## Get diagonals for class accuracy\n",
    "    diag = cm_norm.diagonal()\n",
    "    \n",
    "    # Get the mean of the diagonal values\n",
    "    score = np.mean(diag)\n",
    "    \n",
    "    ## Display Results for the User\n",
    "    print(f'Mean Class Accuracy = {score}')\n",
    "    print(f'Class Accuracy Values:')\n",
    "    print(diag)    \n",
    "\n",
    "    ## Plot the confusion matrix.\n",
    "    ji.plot_confusion_matrix(cm,normalize=True)\n",
    "\n",
    "    # return the score \n",
    "    return score\n",
    "```        \n",
    "        \n",
    "        \n",
    "3. **How do you use it?**\n",
    "    - When instantiating GridSearchCV pass your function as the `scoring=` parameter, wrapped in the  `sklearn.metrics.make_scorer` function.\n",
    "  \n",
    "\n",
    "```python\n",
    "## Using custom scoring function\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "grid = GridSearchCV(estimator=neural_network, \n",
    "                    param_grid=params_to_test,\n",
    "                   scoring=make_scorer(my_custom_scorer))\n",
    "                    \n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T17:11:58.415288Z",
     "start_time": "2020-02-12T17:11:58.408026Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "def my_custom_scorer(y_true,y_pred,scoring='accuracy',verbose=True):\n",
    "    \"\"\"My custom score function to use with sklearn's GridSearchCV\n",
    "    Maximizes the average accuracy per class using a normalized confusion matrix\"\"\"\n",
    "    import sklearn.metrics as metrics\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import numpy as np\n",
    "#     import functions_combined_BEST as ji    \n",
    "#     print('\\n\\n')\n",
    "    print('---'*20)\n",
    "    ## Flatten one-hot encoded target columns into 1 column for sklearn functions\n",
    "    if y_true.ndim>1 or y_pred.ndim>1:\n",
    "        \n",
    "        ## reduce dimensions of y_train and y_test\n",
    "        if y_true.ndim>1:            \n",
    "            y_true = y_true.argmax(axis=1)\n",
    "        \n",
    "        if y_pred.ndim>1:\n",
    "            y_pred = y_pred.argmax(axis=1)\n",
    "\n",
    "    if scoring=='accuracy':\n",
    "        score =  metrics.accuracy_score(y_true,y_pred)\n",
    "    \n",
    "    elif scoring=='class_accuracy':\n",
    "        # Get confusion matr\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "        # Normalize confusion matrix\n",
    "        cm_norm = cm.astype('float')/cm.sum(axis=1)[:,np.newaxis]\n",
    "\n",
    "        ## Get diagonals for class accuracy\n",
    "        diag = cm_norm.diagonal()\n",
    "\n",
    "        # Get the mean of the diagonal values\n",
    "        score = np.mean(diag)\n",
    "\n",
    "        ## Display Results for the User\n",
    "        if verbose:\n",
    "            print(f'Mean Class Accuracy = {score}')\n",
    "            print(f'Class Accuracy Values:')\n",
    "            print(diag)    \n",
    "\n",
    "#     ## Plot the confusion matrix.\n",
    "#     fig = plot_confusion_matrix((y_true,y_pred))\n",
    "#     display(fig)\n",
    "    evaluate_model(y_true,y_pred)\n",
    " \n",
    "    print('---'*20)\n",
    "    print('\\n\\n')\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T17:14:33.427128Z",
     "start_time": "2020-02-12T17:12:09.549338Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "classifier = KerasClassifier(build_fn = build_model)\n",
    "parameters = {'batch_size': [25, 32],\n",
    "              'epochs': [10, 15],\n",
    "              'optimizer': ['adam', 'rmsprop']}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = classifier,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = make_scorer(my_custom_scorer),#'accuracy',\n",
    "                           cv = 5)\n",
    "\n",
    "timer = Timer()\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "best_parameters = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_\n",
    "timer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T17:06:17.108505Z",
     "start_time": "2020-02-12T17:06:17.106392Z"
    }
   },
   "source": [
    "## ADDING EMAIL NOTIFICATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T17:14:40.734117Z",
     "start_time": "2020-02-12T17:14:40.730800Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_secret_password(file='/Users/jamesirving/.secret/gmail.json'):\n",
    "    with open(file) as file:\n",
    "        import json\n",
    "        gmail = json.loads(file.read())\n",
    "    # email_notification()\n",
    "    print(gmail.keys())\n",
    "    return gmail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T17:14:50.319124Z",
     "start_time": "2020-02-12T17:14:50.314907Z"
    }
   },
   "outputs": [],
   "source": [
    "gmail = get_secret_password()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T17:14:52.371628Z",
     "start_time": "2020-02-12T17:14:52.363711Z"
    }
   },
   "outputs": [],
   "source": [
    "def email_notification(password_obj=None,subject='GridSearch Finished',msg='The GridSearch is now complete.'):\n",
    "    \"\"\"Sends email notification from gmail account using previously encrypyted password  object (an instance\n",
    "    of EncrypytedPassword). \n",
    "    Args:\n",
    "        password_obj (EncryptedPassword object): EncryptedPassword object with username/password.\n",
    "        subject (str):Text for subject line.\n",
    "        msg (str): Text for body of email. \n",
    "\n",
    "    Returns:\n",
    "        Prints `Email sent!` if email successful. \n",
    "    \"\"\"\n",
    "    ## Display instructions if no password_obj \n",
    "    if password_obj is None:\n",
    "        print('Must pass an EncrypytedPassword object.')\n",
    "        print('>> pwd_obj = EncryptedPassword(username=\"my_username\",password=\"my_password\")')\n",
    "        print('>> send_email(encrypted_password_obj=pwd_obj)')\n",
    "        raise Exception('Must pass an EncryptedPassword.')\n",
    "    if isinstance(password_obj,dict):\n",
    "        gmail_user = password_obj['username']\n",
    "        gmail_password = password_obj['password']\n",
    "    else:\n",
    "        \n",
    "        ## Get username and password from password_obj\n",
    "        gmail_user = password_obj.username\n",
    "        gmail_password = password_obj._password_\n",
    "        \n",
    "    \n",
    "    # import required packages\n",
    "    import smtplib\n",
    "    from email.mime.multipart import MIMEMultipart\n",
    "    from email.mime.text import MIMEText\n",
    "    from email import encoders\n",
    "    \n",
    "\n",
    "    ## WRITE EMAIL\n",
    "    message = MIMEMultipart()\n",
    "    message['Subject'] =subject\n",
    "    message['To'] = gmail_user\n",
    "    message['From'] = gmail_user\n",
    "    body = msg\n",
    "    message.attach(MIMEText(body,'plain'))\n",
    "    text_message = message.as_string()\n",
    "\n",
    "\n",
    "    # Send email request\n",
    "    try:\n",
    "        with  smtplib.SMTP_SSL('smtp.gmail.com',465) as server:\n",
    "            \n",
    "            server.login(gmail_user,gmail_password)\n",
    "            server.sendmail(gmail_user,gmail_user, text_message)\n",
    "            server.close()\n",
    "            print('Email sent!')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('Something went wrong')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T17:17:42.583444Z",
     "start_time": "2020-02-12T17:17:20.926582Z"
    }
   },
   "outputs": [],
   "source": [
    "## PUTTINGN IT ALL TOGETHER\n",
    "gmail = get_secret_password()\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "classifier = KerasClassifier(build_fn = build_model)\n",
    "parameters = {'batch_size': [25, 32],\n",
    "              'epochs': [10, 15],\n",
    "              'optimizer': ['adam', 'rmsprop']}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = classifier,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = make_scorer(my_custom_scorer),#'accuracy',\n",
    "                           cv = 5)\n",
    "\n",
    "timer = Timer()\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "best_parameters = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_\n",
    "timer.stop()\n",
    "email_notification(gmail)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T16:28:26.717561Z",
     "start_time": "2020-02-12T16:28:26.643184Z"
    }
   },
   "outputs": [],
   "source": [
    "# params_to_test = {'n_units':[(50,25,7),(100,50,7)],\n",
    "#                   'optimizer':['adam','rmsprop','adadelta'],\n",
    "#                   'activation':['linear','relu','tanh'],\n",
    "#                   'final_activation':['softmax']}\n",
    "# grid = GridSearchCV(estimator=neural_network,param_grid=params_to_test)\n",
    "\n",
    "\n",
    "# grid_result = grid.fit(X_train, y_train)\n",
    "# best_params = grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_custom_scorer(y_true,y_pred):\n",
    "    \"\"\"My custom score function to use with sklearn's GridSearchCV\n",
    "    Maximizes the average accuracy per class using a normalized confusion matrix\"\"\"\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import numpy as np\n",
    "    import functions_combined_BEST as ji    \n",
    "\n",
    "    ## Flatten one-hot encoded target columns into 1 column for sklearn functions\n",
    "    if y_true.ndim>1 or y_pred.ndim>1:\n",
    "        \n",
    "        ## reduce dimensions of y_train and y_test\n",
    "        if y_true.ndim>1:            \n",
    "            y_true = y_true.argmax(axis=1)\n",
    "        \n",
    "        if y_pred.ndim>1:\n",
    "            y_pred = y_pred.argmax(axis=1)\n",
    "\n",
    "    \n",
    "     # Get confusion matrx\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Normalize confusion matrix\n",
    "    cm_norm = cm.astype('float')/cm.sum(axis=1)[:,np.newaxis]\n",
    "\n",
    "    ## Get diagonals for class accuracy\n",
    "    diag = cm_norm.diagonal()\n",
    "    \n",
    "    # Get the mean of the diagonal values\n",
    "    score = np.mean(diag)\n",
    "    \n",
    "    ## Display Results for the User\n",
    "    print(f'Mean Class Accuracy = {score}')\n",
    "    print(f'Class Accuracy Values:')\n",
    "    print(diag)    \n",
    "\n",
    "    ## Plot the confusion matrix.\n",
    "    ji.plot_confusion_matrix(cm,normalize=True)\n",
    "\n",
    "    # return the score \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sidebar: Augmenting Image Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 💡 Data Augmentation (not covered in class)\n",
    "- Simplest way to reduce overfitting is to increase the size of the training data.\n",
    "- Difficult to do with large datasets, but can be implemented with images as shown below:\n",
    "- **For augmenting image data:**\n",
    "    - Can alter the images already present in the training data by shifting, shearing, scaling, rotating.<br><br> <img src =\"https://www.dropbox.com/s/9i1hl3quwo294jr/data_augmentation_example.png?raw=1\" width=300>\n",
    "    - This usually provides a big leap in improving the accuracy of the model. It can be considered as a mandatory trick in order to improve our predictions.\n",
    "\n",
    "- **In Keras:**\n",
    "    - `ImageDataGenerator` contains several augmentations available.\n",
    "    - Example below:\n",
    "    \n",
    "```python\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(horizontal flip=True)\n",
    "datagen.fit(train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF 02-11-2020 STUDY GROUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T16:28:31.958107Z",
     "start_time": "2020-02-12T16:28:00.245Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T17:02:40.083379Z",
     "start_time": "2020-02-11T17:02:40.062684Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T16:52:34.336107Z",
     "start_time": "2020-02-11T16:52:13.048Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C3U8KA5IPuXV"
   },
   "source": [
    "\n",
    "### Tools & Applications introduced\n",
    "- **Numpy Functions**\n",
    "    - To unrow a 3-D image ( row, cols, RGB):\n",
    "        - `image_array.shape # returns (790, 64,64,3) - 790 imgs, size = 64,64, RGB=3\n",
    "        - `img_unrow = img.reshape(len(image_array), -1).T`\n",
    "            - -1 takes care of the remaining dims\n",
    "    - To increase the 'rank' of a vector (the first dimension) \n",
    "        - `np.reshape(vector, (1,len(image_array))`\n",
    "- **Keras**\n",
    "    - `from keras import models, layers, optimizers`\n",
    "    - `keras.preprocessing.image` \n",
    "    - `keras.preprocessing.text`\n",
    "- `from matplotlib.pyplot import imread, imshow`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GKw2nWdMDKR6"
   },
   "source": [
    "## Section 42: Network Regularlization & Normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lbzweQKTDPTl"
   },
   "source": [
    "## Overview - Regularization\n",
    "- Bias vs variance trade-off\n",
    "- Using test, train, and vali splits. \n",
    "- Prevent overfitting by adding regularization methods (L1, L2, dropout)\n",
    "- Optimizing and training time reduction by normalizing inputs\n",
    "    - Normalizing inputs can drasticaly decrease computation time, and prevent vanishing/exploding graidents. \n",
    "    \n",
    "### Hyperparameters to Tune\n",
    "- Number of hidden units\n",
    "- Number of layers\n",
    "- Learning rate ( $\\alpha$)\n",
    "- Activation function\n",
    "\n",
    "### Training, Validation, and Test Sets\n",
    "- The fact that there are so many hyperparameters to tune calls for a formalized and unbiased approach to testing/training sets.\n",
    "- We will use 3 sets when running, selecting, and validating a model:\n",
    "    - Training set: for training the alogrithm\n",
    "    - Validation set: to decide which model will be the final one after parameter tuning\n",
    "    - Testing set: after choosing final  the final model, use the test set for an inbiased estimate of performance. \n",
    "- Set sizes:\n",
    "    - With big data, your dev and test sets don't necessarily need to be 20-30% of all the data. \n",
    "    - You can choose test and hold-out sets that are of size 1-5%. \n",
    "        - eg. 96% train, 2% hold-out, 2% test set. \n",
    "    - It is **VERY IMPORTANT** to make sure holdout and test sample come from the same distribution: eg. same resolution of santa pictures. \n",
    "    \n",
    "### Bias vs Variance \n",
    "- A model with high bias may result in underfitting.\n",
    "    - <img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-42-02-tuning-neural-networks-with-regularization-online-ds-ft-021119/master/figures/underfitting.png\" width=200>\n",
    "- A model with high variance may result in overfitting. \n",
    "    - <img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-42-02-tuning-neural-networks-with-regularization-online-ds-ft-021119/master/figures/overfitting.png\" width=200>\n",
    "\n",
    "- In deep learning, there is less of a bias-variance trad-off vs simpler models. \n",
    "\n",
    "**Rules of thumb re: bias/variance trade-off:**\n",
    "\n",
    "| High Bias? (training performance) | high variance? (validation performance)  |\n",
    "|---------------|-------------|\n",
    "| Use a bigger network|    More data     |\n",
    "| Train longer | Regularization   |\n",
    "| Look for other existing NN architextures |Look for other existing NN architextures |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BlPs5Wd8GrNg"
   },
   "source": [
    "### L1 & L2 Regularlization\n",
    "- These methods of regularizaiton do so by penalizing coefficients(regression) or weights(neural networks),\n",
    "    - L1 & L2 exist in regression models as well. There, L1='Lasso Regressions' , L2='Ridge regression'\n",
    "\n",
    "- **L1 & L2 regularization add a term to the cost function.**\n",
    "\n",
    "$$Cost function = Loss (say, binary cross entropy) + Regularization term$$\n",
    "\n",
    "$$ J (w^{[1]},b^{[1]},...,w^{[L]},b^{[L]}) = \\dfrac{1}{m} \\sum^m_{i=1}\\mathcal{L}(\\hat y^{(i)}, y^{(i)})+ \\dfrac{\\lambda}{2m}\\sum^L_{l=1}||w^{[l]}||^2$$\n",
    "\n",
    "    - where $\\lambda$ is the regularization parameter. \n",
    "    - The difference between  L1 vs L2 is that L1 is just the sum of the weights whereas L2 is the sum of the _square_of the weights.  \n",
    "\n",
    "- **L1 Regularization:**\n",
    "    $$ Cost function = Loss + \\frac{\\lambda}{2m} * \\sum ||w||$$\n",
    "    - Uses the absolute value of weights and may reduce the weights down to 0. \n",
    "    \n",
    "        \n",
    "- **L2 Regularization:**:\n",
    "    $$ Cost function = Loss + \\frac{\\lambda}{2m} * \\sum ||w||^2$$\n",
    "    - Also known as weight decay, as it forces weights to decay towards zero, but never exactly 0.. \n",
    "    \n",
    "-  Regularization term $||w^{[l]}||^2 _F$  is  A.K.A. The Frobenius Norm\n",
    "    - $||w^{[l]}||^2 = \\sum^{n^{[l-1]}}_{i=1} \\sum^{n^{[l]}}_{j=1} (w_{ij}^{[l]})^2$\n",
    "\n",
    "    \n",
    "- **CHOOSING L1 OR L2:**\n",
    "    - L1 is very useful when trying to compress a model. (since weights can decreae to 0)\n",
    "    - L2 is generally preferred otherwise.\n",
    "    \n",
    "- **USING L1/L2 IN KERAS:**\n",
    "    - Add a kernel_regulaizer to a  layer.\n",
    "```python \n",
    "from keras import regularizers\n",
    "model.add(Dense(64, input_dim=64, kernel_regularizer=regularizers.l2(0.01))\n",
    "```\n",
    "    - here 0.01 = $\\lambda$\n",
    "\n",
    "### Dropout Regularization\n",
    "- Uses a specified probablity to random leave out a node from a ---epoch?\n",
    "\n",
    "\n",
    "- **USING DROPOUT IN KERAS:**\n",
    "    - Dropout layers are located in keras.layers.core \n",
    "    - Specify probably of being exlcuded/dropped out.\n",
    "```python\n",
    "from keras.layers.core import Dropout\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(output_dim=hidden1_num_units, input_dim=input_num_units, activation='relu'))\n",
    "model.add(layers.core.Dropout(Dropout(0.25))                              \n",
    "```\n",
    "\n",
    "### Early Stopping (not covered in class)\n",
    "- Monitor performance for decrease or plateau in performance, terminate process when given criteria is reached.\n",
    "\n",
    "- **In Keras:**\n",
    "    - Can be applied using the [callbacks function](https://keras.io/callbacks/)\n",
    "```python    \n",
    "from keras.callbacks import EarlyStopping\n",
    "EarlyStopping(monitor='val_err', patience=5)\n",
    "```\n",
    "    - 'Monitor' denotes quanitity to check\n",
    "    - 'val_err' denotes validation error\n",
    "    - 'pateience' denotes # of epochs without improvement before stopping.\n",
    "        - Be careful, as sometimes models _will_ continue to improve after a stagnant period\n",
    "\n",
    "### Reference Links I found:\n",
    "- https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/\n",
    "- http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOW TO: Custom Scoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T16:28:31.958642Z",
     "start_time": "2020-02-12T16:28:00.255Z"
    },
    "code_folding": [
     0,
     8
    ]
   },
   "outputs": [],
   "source": [
    "def get_secret_password(file='/Users/jamesirving/.secret/gmail.json'):\n",
    "    with open(file) as file:\n",
    "        import json\n",
    "        gmail = json.loads(file.read())\n",
    "    # email_notification()\n",
    "    print(gmail.keys())\n",
    "    return gmail\n",
    "\n",
    "def email_notification(password_obj=None,subject='GridSearch Finished',msg='The GridSearch is now complete.'):\n",
    "    \"\"\"Sends email notification from gmail account using previously encrypyted password  object (an instance\n",
    "    of EncrypytedPassword). \n",
    "    Args:\n",
    "        password_obj (EncryptedPassword object): EncryptedPassword object with username/password.\n",
    "        subject (str):Text for subject line.\n",
    "        msg (str): Text for body of email. \n",
    "\n",
    "    Returns:\n",
    "        Prints `Email sent!` if email successful. \n",
    "    \"\"\"\n",
    "    ## Display instructions if no password_obj \n",
    "    if password_obj is None:\n",
    "        print('Must pass an EncrypytedPassword object.')\n",
    "        print('>> pwd_obj = EncryptedPassword(username=\"my_username\",password=\"my_password\")')\n",
    "        print('>> send_email(encrypted_password_obj=pwd_obj)')\n",
    "        raise Exception('Must pass an EncryptedPassword.')\n",
    "    if isinstance(password_obj,dict):\n",
    "        gmail_user = password_obj['username']\n",
    "        gmail_password = password_obj['password']\n",
    "    else:\n",
    "        \n",
    "        ## Get username and password from password_obj\n",
    "        gmail_user = password_obj.username\n",
    "        gmail_password = password_obj._password_\n",
    "        \n",
    "    \n",
    "    # import required packages\n",
    "    import smtplib\n",
    "    from email.mime.multipart import MIMEMultipart\n",
    "    from email.mime.text import MIMEText\n",
    "    from email import encoders\n",
    "    \n",
    "\n",
    "    ## WRITE EMAIL\n",
    "    message = MIMEMultipart()\n",
    "    message['Subject'] =subject\n",
    "    message['To'] = gmail_user\n",
    "    message['From'] = gmail_user\n",
    "    body = msg\n",
    "    message.attach(MIMEText(body,'plain'))\n",
    "    text_message = message.as_string()\n",
    "\n",
    "\n",
    "    # Send email request\n",
    "    try:\n",
    "        with  smtplib.SMTP_SSL('smtp.gmail.com',465) as server:\n",
    "            \n",
    "            server.login(gmail_user,gmail_password)\n",
    "            server.sendmail(gmail_user,gmail_user, text_message)\n",
    "            server.close()\n",
    "            print('Email sent!')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('Something went wrong')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T16:28:31.959437Z",
     "start_time": "2020-02-12T16:28:00.258Z"
    }
   },
   "outputs": [],
   "source": [
    "# gmail = get_secret_password()\n",
    "# email_notification(gmail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T16:28:31.960447Z",
     "start_time": "2020-02-12T16:28:00.260Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def my_custom_scorer(y_true,y_pred):\n",
    "    \"\"\"My custom score function to use with sklearn's GridSearchCV\n",
    "    Maximizes the average accuracy per class using a normalized confusion matrix\n",
    "    [i] Note: To use my_custom_scorer in GridSearch:\n",
    "    >> from sklearn.metrics import make_scorer\n",
    "    >> grid = GridSearch(estimator, parameter_grid)\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import make_scorer,confusion_matrix\n",
    "    import numpy as np\n",
    "    import functions_combined_BEST as ji    \n",
    "\n",
    "    # set labels for confusion matrix\n",
    "    labels = ['Decrease','No Change', 'Increase']\n",
    "\n",
    "    \n",
    "    ## If y_true is a multi-column one-hotted target\n",
    "    if y_true.ndim>1 or y_pred.ndim>1:\n",
    "\n",
    "        ## reduce dimensions of y_train and y_test\n",
    "        if y_true.ndim>1:            \n",
    "            y_true = y_true.argmax(axis=1)\n",
    "            \n",
    "        if y_pred.ndim>1:\n",
    "            y_pred = y_pred.argmax(axis=1)\n",
    "\n",
    "    \n",
    "    ## Get confusion matrx\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    ## Normalize confusion matrix\n",
    "    cm_norm = cm.astype('float')/cm.sum(axis=1)[:,np.newaxis]\n",
    "\n",
    "    ## Get diagonals for class accuracy\n",
    "    diag = cm_norm.diagonal()\n",
    "    score = np.mean(diag)\n",
    "    \n",
    "    \n",
    "    ## Display results for user\n",
    "    print(f'Mean Class Accuracy = {score}')\n",
    "    print(f'Class Accuracy Values:')\n",
    "    print(diag)    \n",
    "\n",
    "    ## Plot confusion matrix\n",
    "    ji.plot_confusion_matrix(cm,normalize=True,classes=labels);\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y0QuBXKwRlbc"
   },
   "source": [
    "## Overview - Network Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y0QuBXKwRlbc"
   },
   "source": [
    "### Normalization\n",
    "- Normalizing to a consistent scale (typically 0 to 1) improves performance, but also ensures the process will converge to a stable solution. \n",
    "\n",
    "- Methods:\n",
    "    - Z-Score (subtracting mean, normalize by standard deviation)\n",
    "    \n",
    "#### Reference Links\n",
    "- https://www.coursera.org/lecture/deep-neural-network/normalizing-inputs-lXv6U\n",
    "\n",
    "### Changing Initial Parameters\n",
    "- The more input features into layer $l$, the smaller we want each weight $w_i$ to be.\n",
    "- Rule of thumb:\n",
    "    - $Var(w_i) = 1/n$ or $2/n$\n",
    "- A common initilization strategy for the relu activation functions is:\n",
    "\n",
    "    * $w^{[l]}$ `= np.random.randn(shape)*np.sqrt(2/n_(l-1))`\n",
    "    \n",
    "## Optimization:\n",
    "Alternatives to gradient descent that do not oscillate as much as g.d.:\n",
    "### Gradient Descent with Momentum:\n",
    "- Comutes an exponentially weighted average of the gradients to use.\n",
    "    - will dampen oscillations and improve performance.\n",
    "- How to:\n",
    "    -  Calculate current batch's moving averages for the derivatives of $W$ and$b$\n",
    "        - Compute $V_{dw} = \\beta V_{dw} + (1-\\beta)dW$\n",
    "        - $V_{db} = \\beta V_{db} + (1-\\beta)db$ \n",
    "        - So updated terms become\n",
    "            - $W:= W- \\alpha Vdw$\n",
    "            -$b:= b- \\alpha Vdb$\n",
    "    -  Generally, $\\beta=0.9$ is a good hyperparameter value.\n",
    "    \n",
    "### RMSprop\n",
    "- \"Root mean square\" prop\n",
    "- Slow down learning in one direction and speed it up in another.\n",
    "    - In the direction where we want to learn fast, the corresponding S will be small, so dividing by a small number. \n",
    "    - In the direction where we will want to learn slow, the corresponding S will be relatively large, and updates will be smaller. \n",
    "- How to:\n",
    "    - On each iteration, use exponentially weighted average again:\n",
    "        - exponentially weighted average of the squares of the derivatives\n",
    "        - $S_{dw} = \\beta S_{dw} + (1-\\beta)dW^2$\n",
    "        - $S_{db} = \\beta S_{dw} + (1-\\beta)db^2$\n",
    "        - So that:\n",
    "            - $W:= W- \\alpha \\dfrac{dw}{\\sqrt{S_{dw}}}$\n",
    "            - $b:= b- \\alpha \\dfrac{db}{\\sqrt{S_{db}}}$\n",
    "    - Often, add small $\\epsilon$ in the denominator to make sure that you don't end up dividing by 0.\n",
    "\n",
    "\n",
    "### Adam Optimization Algorithm\n",
    "- Adaptive Moment Estimation - essentially combines both methods above.\n",
    "- Works very well in most situations.\n",
    "- How to: \n",
    "    - Initialize: $V_{dw}=0, S_{dw}=0, V_{db}=0, S_{db}=0$.\n",
    "    - For each teration: compute $dW, db$ using the current mini-batch.\n",
    "        -  $V_{dw} = \\beta_1 V_{dw} + (1-\\beta_1)dW$, $V_{db} = \\beta_1 V_{db} + (1-\\beta_1)db$ \n",
    "        -  $S_{dw} = \\beta_2 S_{dw} + (1-\\beta_2)dW^2$, $S_{db} = \\beta_2 S_{db} + (1-\\beta_2)db^2$ \n",
    "        \n",
    "- As with  momentum and then RMSprop. We need to perform a correction! This is sometimes also done in RSMprop, but definitely here too.\n",
    "    - $V^{corr}_{dw}= \\dfrac{V_{dw}}{1-\\beta_1^t}$, $V^{corr}_{db}= \\dfrac{V_{db}}{1-\\beta_1^t}$\n",
    "\n",
    "    - $S^{corr}_{dw}= \\dfrac{S_{dw}}{1-\\beta_2^t}$, $S^{corr}_{db}= \\dfrac{S_{db}}{1-\\beta_2^t}$\n",
    "\n",
    "    - $W:= W- \\alpha \\dfrac{V^{corr}_{dw}}{\\sqrt{S^{corr}_{dw}+\\epsilon}}$ and\n",
    "\n",
    "    - $b:= b- \\alpha \\dfrac{V^{corr}_{db}}{\\sqrt{S^{corr}_{db}+\\epsilon}}$ \n",
    "\n",
    "\n",
    "### Learning Rate Decay\n",
    "- Learning rate decreases across epochs.\n",
    "    - $\\alpha = \\dfrac{1}{1+\\text{decay_rate * epoch_nb}}* \\alpha_0$\n",
    "\n",
    "- other methods:\n",
    "    - $\\alpha = 0.97 ^{\\text{epoch_nb}}* \\alpha_0$ (or exponential decay)<br>OR:\n",
    "    - $\\alpha = \\dfrac{k}{\\sqrt{\\text{epoch_nb}}}* \\alpha_0$<br> OR:\n",
    "    - Manual decay.\n",
    "    \n",
    "    \n",
    "    \n",
    "### HYPERPARAMETER TUNING:\n",
    "Most important:\n",
    "- $\\alpha$\n",
    "\n",
    "Important next:\n",
    "- $\\beta$ (momentum)\n",
    "- Number of hidden units\n",
    "- mini-batch-size\n",
    "\n",
    "Finally:\n",
    "- Number of layers\n",
    "- Learning rate decay\n",
    "\n",
    "Almost never tuned:\n",
    "- $\\beta_1$, $\\beta_2$, $\\epsilon$ (Adam)\n",
    "\n",
    "- Tip: Don't use a grid, because hard to say in advance which hyperparameters will be important.\n",
    "\n",
    "\n",
    "### OPTIMIZAITON REFS:\n",
    "- https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "- https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
    "- https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/\n",
    "- https://stackoverflow.com/questions/37232782/nan-loss-when-training-regression-network https://www.springboard.com/blog/free-public-data-sets-data-science-project/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of James' Study Group Notes - Section 41-42 (condensed version).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
