{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 43: Foundations of Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP & Word Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Natural Language Processing_**, or **_NLP_**, is the study of how computers can interact with humans through the use of human language.  Although this is a field that is quite important to Data Scientists, it does not belong to Data Science alone.  NLP has been around for quite a while, and sits at the intersection of *Computer Science*, *Artificial Intelligence*, *Linguistics*, and *Information Theory*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Demonstration:**\n",
    "    - [Google Duplex AI Assistant](https://youtu.be/D5VN56jQMWM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preparing text data requires more processing than normal data.\n",
    "- In addition to cleaning the text itself to remove meaningless words, we have to convert our text data into numeric form for our machine learning models to analyze.\n",
    "- Text data must be cleaned and vectorized before we can use it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Vocabulary\n",
    "- Corpus\n",
    "    - Body of text\n",
    "    \n",
    "- Bag of Words\n",
    "    - Collection of all words from a corpus.\n",
    "\n",
    "    \n",
    "- Stopwords\n",
    "- Tokenization\n",
    "    - Separating long strings into single-words in a list.\n",
    "    \n",
    "- Stemming \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-students/dsc-nlp-and-word-vectorization-online-ds-ft-100719/master/images/new_stemming.png\" width=40%>\n",
    "\n",
    "- Lemmatization\n",
    "\n",
    "|   Word   |  Stem | Lemma |\n",
    "|:--------:|:-----:|:-----:|\n",
    "|  Studies | Studi | Study |\n",
    "| Studying | Study | Study |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Count vectorization\n",
    "- Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "    -  Used for multiple texts\n",
    "    \n",
    "    \n",
    "**_Term Frequency_** is calculated with the following formula:\n",
    "\n",
    "$$\\large \\text{Term Frequency}(t) = \\frac{\\text{number of times it appears in a document}} {\\text{total number of terms in the document}} $$ \n",
    "\n",
    "**_Inverse Document Frequency_** is calculated with the following formula:\n",
    "\n",
    "$$\\large IDF(t) = log_e(\\frac{\\text{Total Number of Documents}}{\\text{Number of Documents with it in it}})$$\n",
    "\n",
    "The **_TF-IDF_** value for a given word in a given document is just found by multiplying the two!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T02:35:40.805046Z",
     "start_time": "2020-01-31T02:35:35.467359Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fsds_1007219  v0.7.4 loaded.  Read the docs: https://fsds.readthedocs.io/en/latest/ \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_5eca02c8_43d2_11ea_9f1e_acde48001122\" ><caption>Loaded Packages and Handles</caption><thead>    <tr>        <th class=\"col_heading level0 col0\" >Handle</th>        <th class=\"col_heading level0 col1\" >Package</th>        <th class=\"col_heading level0 col2\" >Description</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_5eca02c8_43d2_11ea_9f1e_acde48001122row0_col0\" class=\"data row0 col0\" >dp</td>\n",
       "                        <td id=\"T_5eca02c8_43d2_11ea_9f1e_acde48001122row0_col1\" class=\"data row0 col1\" >IPython.display</td>\n",
       "                        <td id=\"T_5eca02c8_43d2_11ea_9f1e_acde48001122row0_col2\" class=\"data row0 col2\" >Display modules with helpful display and clearing commands.</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_5eca02c8_43d2_11ea_9f1e_acde48001122row1_col0\" class=\"data row1 col0\" >fs</td>\n",
       "                        <td id=\"T_5eca02c8_43d2_11ea_9f1e_acde48001122row1_col1\" class=\"data row1 col1\" >fsds_100719</td>\n",
       "                        <td id=\"T_5eca02c8_43d2_11ea_9f1e_acde48001122row1_col2\" class=\"data row1 col2\" >Custom data science bootcamp student package</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_5eca02c8_43d2_11ea_9f1e_acde48001122row2_col0\" class=\"data row2 col0\" >mpl</td>\n",
       "                        <td id=\"T_5eca02c8_43d2_11ea_9f1e_acde48001122row2_col1\" class=\"data row2 col1\" >matplotlib</td>\n",
       "                        <td id=\"T_5eca02c8_43d2_11ea_9f1e_acde48001122row2_col2\" class=\"data row2 col2\" >Matplotlib's base OOP module with formatting artists</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_5eca02c8_43d2_11ea_9f1e_acde48001122row3_col0\" class=\"data row3 col0\" >plt</td>\n",
       "                        <td id=\"T_5eca02c8_43d2_11ea_9f1e_acde48001122row3_col1\" class=\"data row3 col1\" >matplotlib.pyplot</td>\n",
       "                        <td id=\"T_5eca02c8_43d2_11ea_9f1e_acde48001122row3_col2\" class=\"data row3 col2\" >Matplotlib's matlab-like plotting module</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_5eca02c8_43d2_11ea_9f1e_acde48001122row4_col0\" class=\"data row4 col0\" >np</td>\n",
       "                        <td id=\"T_5eca02c8_43d2_11ea_9f1e_acde48001122row4_col1\" class=\"data row4 col1\" >numpy</td>\n",
       "                        <td id=\"T_5eca02c8_43d2_11ea_9f1e_acde48001122row4_col2\" class=\"data row4 col2\" >scientific computing with Python</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_5eca02c8_43d2_11ea_9f1e_acde48001122row5_col0\" class=\"data row5 col0\" >pd</td>\n",
       "                        <td id=\"T_5eca02c8_43d2_11ea_9f1e_acde48001122row5_col1\" class=\"data row5 col1\" >pandas</td>\n",
       "                        <td id=\"T_5eca02c8_43d2_11ea_9f1e_acde48001122row5_col2\" class=\"data row5 col2\" >High performance data structures and tools</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_5eca02c8_43d2_11ea_9f1e_acde48001122row6_col0\" class=\"data row6 col0\" >sns</td>\n",
       "                        <td id=\"T_5eca02c8_43d2_11ea_9f1e_acde48001122row6_col1\" class=\"data row6 col1\" >seaborn</td>\n",
       "                        <td id=\"T_5eca02c8_43d2_11ea_9f1e_acde48001122row6_col2\" class=\"data row6 col2\" >High-level data visualization library based on matplotlib</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1a1b29def0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[i] Pandas .iplot() method activated.']\n"
     ]
    }
   ],
   "source": [
    "!pip install -U fsds_100719\n",
    "from fsds_100719.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T02:37:13.734672Z",
     "start_time": "2020-01-31T02:37:13.232246Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/jirvingphd/capstone-project-using-trumps-tweets-to-predict-stock-market/master/data/trump_tweets_12012016_to_01012020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T02:38:38.293110Z",
     "start_time": "2020-01-31T02:38:38.288576Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://t.co/EVAEYD1AgV',\n",
       " 'HAPPY NEW YEAR!',\n",
       " 'Our fantastic First Lady! https://t.co/6iswto4WDI',\n",
       " 'RT @DanScavino: https://t.co/CJRPySkF1Z',\n",
       " 'RT @SenJohnKennedy: I think Speaker Pelosi is having 2nd thoughts about impeaching the President. The Senate should get back to work on USMâ€¦',\n",
       " 'Thank you Steve. The greatest Witch Hunt in U.S. history! https://t.co/I3bSNVp6gC',\n",
       " 'RT @ThisWeekABC: Sen. Ron Johnson says charges against Pres. Trump are \"pretty thin gruel\" and Speaker Nancy Pelosi\\'s decision to withholdâ€¦',\n",
       " \"RT @SenJohnKennedy: The Senate needs to reauthorize the Violence Against Women Act and I am proud to cosponsor @SenJoniErnst's bill that gâ€¦\",\n",
       " 'RT @LindseyGrahamSC: To our Iraqi allies:This is your moment to convince the American people the US-Iraq relationship is meaningful to yoâ€¦',\n",
       " 'RT @LindseyGrahamSC: President Trump unlike President Obama will hold you accountable for threats against Americans and hit you where itâ€¦']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = list(df['text'].values)\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T02:53:38.163618Z",
     "start_time": "2020-01-31T02:53:38.161231Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T02:53:38.722938Z",
     "start_time": "2020-01-31T02:53:38.718441Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all the stop words in the English language\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list.remove('until')\n",
    "stopwords_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T02:53:41.294973Z",
     "start_time": "2020-01-31T02:53:41.292733Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords_list += string.punctuation\n",
    "# stopwords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T02:53:41.964691Z",
     "start_time": "2020-01-31T02:53:41.937540Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546e2e0b54834c64824be67296d1d10d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=7032, description='i', max=14065), Output()), _dom_classes=('widget-inteâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from ipywidgets import interact\n",
    "\n",
    "@interact\n",
    "def tokenize_tweet(i=(0,len(corpus)-1)):\n",
    "    print(corpus[i],'\\n')\n",
    "    tokens = word_tokenize(corpus[i])\n",
    "\n",
    "\n",
    "    # It is usually a good idea to lowercase all tokens during this step, as well\n",
    "    stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "    print(tokens,end='\\n\\n')\n",
    "    return print(stopped_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T02:57:27.233652Z",
     "start_time": "2020-01-31T02:57:27.229396Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://t']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Find(string): \n",
    "    # findall() has been used  \n",
    "    # with valid conditions for urls in string \n",
    "    url = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+] |[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', string) \n",
    "    return url \n",
    "Find(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering for Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context-Free Grammers and POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
