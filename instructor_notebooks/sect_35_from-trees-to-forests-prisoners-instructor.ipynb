{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ensembles: From Decision Trees to Extra Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods: Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Ensemble Methods take advantage of the delphic technique (or \"wisdom of crowds\") where the average of multiple independent estimates is usually more consistently accurate than the individual estimates.***\n",
    "\n",
    "\n",
    "\n",
    "> - Bootstrap Aggregation\n",
    "    - Bagging Classifier\n",
    "    - Random Forests\n",
    "- Gradient Boosting:\n",
    "    - Adaboost\n",
    "    - Gradient Boosted Trees\n",
    "- Model Stacking A.K.K. Meta-Ensembling\n",
    "\n",
    "\n",
    "<!---\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/fsds_100719_cohort_notes/master/Flashcards/Ensemble_Methods_web.png\">\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/fsds_100719_cohort_notes/master/Flashcards/Avoid_Overfitting_web.png\">--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap Aggregation (Bagging)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The process for training an ensemble through bootstrap aggregation is as follows:\n",
    "\n",
    "1. Grab a sizable sample from your dataset, with replacement \n",
    "2. Train a classifier on this sample  \n",
    "3. Repeat until all classifiers have been trained on their own sample from the dataset  \n",
    "4. When making a prediction, have each classifier in the ensemble make a prediction \n",
    "5. Aggregate all predictions from all classifiers into a single prediction, using the method of your choice  \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/fsds_100719_cohort_notes/master/Flashcards/Bagging_web.png\">\n",
    "\n",
    "<!---<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-ensemble-methods-online-ds-ft-100719/master/images/new_bagging.png\">--->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Because decision trees are greedy algorithms, every tree given same data would make same conclusions.\n",
    "- In addition to bagging, random forests use **subspace sampling**\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/fsds_100719_cohort_notes/master/Flashcards/Random_Forest_web.png\">\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-students/dsc-random-forests-online-ds-ft-100719/master/images/new_rf-diagram.png\" width=70%>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Benefits and drawbacks\n",
    "\n",
    "Like any algorithm, random forest comes with its own benefits and drawbacks. \n",
    "\n",
    "#### Benefits\n",
    "\n",
    "* **_Strong performance_** Because this is an ensemble algorithm, the model is naturally resistant to noise and variance in the data, and generally tends to perform quite well. \n",
    "\n",
    "* **_Interpretability_**:  each tree in the random forest is a **_Glass-Box Model_** (meaning that the model is interpretable, allowing us to see how it arrived at a certain decision), the overall random forest is, as well! \n",
    "\n",
    "#### Drawbacks\n",
    "\n",
    "* **_Computational complexity_**: On large datasets, the runtime can be quite slow compared to other algorithms.\n",
    "\n",
    "* **_Memory usage_**: Random forests tend to have a larger memory footprint that other models. It's not uncommon to see random forests that were trained on large datasets have memory footprints in the tens, or even hundreds of MB. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting / Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weak learners\n",
    "\n",
    "All the models we've learned so far are **_Strong Learners_** -- models with the goal of doing as well as possible on the classification or regression task they are given. \n",
    "\n",
    "The term **_Weak Learner_** refers to simple models that do only slightly better than random chance. \n",
    "\n",
    "Boosting algorithms start with a single weak learner (usually trees), but technically, any model will do. \n",
    "\n",
    "Boosting works as follows:\n",
    "\n",
    "1. Train a single weak learner  \n",
    "2. Figure out which examples the weak learner got wrong  \n",
    "3. Build another weak learner that focuses on the areas the first weak learner got wrong  \n",
    "4. Continue this process until a predetermined stopping condition is met, such as until a set number of weak learners have been created, or the model's performance has plateaued  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences between Gradient Boosting and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Independent vs iterative\n",
    "    - in Random Forests one tree is unaffacted by another.\n",
    "    - in Boosting mode each tree is iteratively created to address the prior tree's weaknesses.\n",
    "    \n",
    "- Weak vs Strong\n",
    "\n",
    "    - In a random forest, each tree is a strong learner -- they would do just fine as a decision tree on their own.\n",
    "    - In boosting algorithms, trees are artificially limited to a very shallow depth (usually only 1 split) \n",
    "        - to ensure that **each model is only slightly better than random chance**. \n",
    "\n",
    "- Aggregate Predictions:\n",
    "    - in RF each tree votes\n",
    "    - in boosting models trees are given weight for being good at \"hard tasks\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost & Gradient Boosted Trees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost (Adaptive Boosting)\n",
    "- **_Key Takeaway:_** Adaboost creates new classifiers by continually influencing the distribution of the data sampled to train each successive learner. \n",
    "- Uses subsampling with weighted-probabilities for incorrect predictions to be included in subsequent weak learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosted Trees\n",
    "- More advanced form - uses gradient descent.\n",
    "- Trains successive trees on the **residuals**\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-gradient-boosting-and-weak-learners-online-ds-ft-100719/master/images/new_gradient-boosting.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Stacking / Meta-Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model stacking is when you use the predictions of one model as the input to another model.\n",
    "<img src=\"https://burakhimmetoglu.files.wordpress.com/2016/12/workflow.png?w=1140\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Prisoner Recidivism with Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "~~To start, we'll build a decision tree classifier, before taking a look at how we can further improve this algorithm by combining multiple decision trees. As we're feeling woodsy, import and inspect the dataset stored in `mushrooms.csv`.~~\n",
    "    \n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-3-final-project-online-ds-ft-021119/master/LSA_map_with_counties_districts_and_B54A5BBCE4156.jpg\" width=30%>\n",
    "\n",
    "- The provided dataset was too easy to predict and I was getting 100% accuracy with vanilla trees.\n",
    "- Instead using partially-processed version of the Iowa Recidivism Dataset from my Mod 3 Project.\n",
    "    - https://github.com/jirvingphd/dsc-3-final-project-online-ds-ft-021119\n",
    "    \n",
    "- **[Non-Technical Presentation with prior results](https://github.com/jirvingphd/dsc-3-final-project-online-ds-ft-021119/blob/master/Predicting%20Recidivism%20in%20Released%20Prisoner%20in%20Iowa_v2.pdf)**\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vanilla DecisionTreeClassifier\n",
    "    - RandomizedSearchCV best-params DTC\n",
    "- Bagging Classifier\n",
    "- RandomForest\n",
    "- ExtraTreesClassifier\n",
    "    - GridSearchCV\n",
    "- XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:21:50.580363Z",
     "start_time": "2020-01-21T03:21:47.867493Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U fsds_100719\n",
    "from fsds_100719.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:21:50.833021Z",
     "start_time": "2020-01-21T03:21:50.581684Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:21:50.874617Z",
     "start_time": "2020-01-21T03:21:50.834430Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler,LabelEncoder, OneHotEncoder, LabelBinarizer\n",
    "from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier,BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OBTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:21:50.877930Z",
     "start_time": "2020-01-21T03:21:50.875913Z"
    },
    "code_folding": [
     0
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_csv('https://raw.githubusercontent.com/jirvingphd/dsc-lp-TUNING-random-forests-and-grid-search/master/mushrooms.csv')#'mushrooms.csv')\n",
    "# print(df.info())\n",
    "# df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:21:51.501435Z",
     "start_time": "2020-01-21T03:21:50.879052Z"
    }
   },
   "outputs": [],
   "source": [
    "prisoners = \"https://raw.githubusercontent.com/jirvingphd/dsc-3-final-project-online-ds-ft-021119/master/iowa_recidivism_renamed.csv\"\n",
    "df = pd.read_csv(prisoners,index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:21:51.507085Z",
     "start_time": "2020-01-21T03:21:51.503091Z"
    }
   },
   "outputs": [],
   "source": [
    "df= df.drop(columns=['yr_released'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:21:51.521398Z",
     "start_time": "2020-01-21T03:21:51.508484Z"
    }
   },
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:21:51.572655Z",
     "start_time": "2020-01-21T03:21:51.523423Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['sex','race_ethnicity'])\n",
    "df = df.fillna('missing')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:21:51.581168Z",
     "start_time": "2020-01-21T03:21:51.574166Z"
    }
   },
   "outputs": [],
   "source": [
    "df['recidivist'].value_counts(normalize=False,dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:21:51.664623Z",
     "start_time": "2020-01-21T03:21:51.582676Z"
    }
   },
   "outputs": [],
   "source": [
    "## LABEL ENCODING\n",
    "encoder_dict = {}\n",
    "\n",
    "df_le = df.copy()\n",
    "for col in df_le.columns:\n",
    "    encoder_dict[col] = LabelEncoder()\n",
    "    df_le[col] = encoder_dict[col].fit_transform(df[col])\n",
    "df_le.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADDRESSING IMBALANCED CLASSES\n",
    "\n",
    "- Downsample/undersampling to match minority class.\n",
    "- Synthetic Minority Over Sampling Technique (SMOTE)\n",
    "-  Adaptive Synthetic (ADASYN)\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/fsds_100719_cohort_notes/master/Flashcards/Downsampling_web.png\" width=10%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:21:51.797515Z",
     "start_time": "2020-01-21T03:21:51.666423Z"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN,SMOTE\n",
    "\n",
    "df_ohe = pd.get_dummies(df,drop_first=True)#,dummy_na=True)#,dummy_na=True)\n",
    "df_ohe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:21:51.817733Z",
     "start_time": "2020-01-21T03:21:51.798725Z"
    }
   },
   "outputs": [],
   "source": [
    "y = df_ohe['recidivist_Yes'].copy()\n",
    "X = df_ohe.drop(columns=['recidivist_Yes']).copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "print(pd.Series(y_train).value_counts() )\n",
    "pd.Series(y_test).value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:21:55.318977Z",
     "start_time": "2020-01-21T03:21:51.818928Z"
    }
   },
   "outputs": [],
   "source": [
    "smote = SMOTE()\n",
    "X_train, y_train = smote.fit_sample(X_train, y_train) \n",
    "print(pd.Series(y_train).value_counts() )\n",
    "pd.Series(y_test).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:21:55.326023Z",
     "start_time": "2020-01-21T03:21:55.320733Z"
    }
   },
   "outputs": [],
   "source": [
    "df_ohe['recidivist_Yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:21:55.330120Z",
     "start_time": "2020-01-21T03:21:55.327886Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ## Undersampling to match smallest class\n",
    "# df_yes = df.groupby('recidivist').get_group('Yes')\n",
    "# df_no = df.groupby('recidivist').get_group('No')\n",
    "\n",
    "# sample_size = min(len(df_yes),len(df_no))\n",
    "# sample_size\n",
    "\n",
    "# sample_state = 123\n",
    "# np.random.seed(sample_state)\n",
    "\n",
    "# df_samp = pd.concat([df_yes.sample(sample_size,random_state=sample_state),\n",
    "#                 df_no.sample(sample_size,random_state=sample_state)],axis=0)\n",
    "# df_samp['recidivist'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:21:55.333846Z",
     "start_time": "2020-01-21T03:21:55.331731Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# y_resampled = df_ohe['recidivist_Yes']\n",
    "# X_resampled = df_ohe.drop(columns=['recidivist_Yes'])\n",
    "# # y = df_le['recidivist']\n",
    "# # X = df_le.drop(columns=['recidivist'])\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_resampled,y_resampled)\n",
    "# X_train.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:21:55.337730Z",
     "start_time": "2020-01-21T03:21:55.335612Z"
    }
   },
   "outputs": [],
   "source": [
    "# encoder_dict['recidivist'].inverse_transform([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining the Problem & Fitting a Decision Tree\n",
    "\n",
    "Now, import the necessary packages and fit a decision tree to predict whether or not a mushroom is poisonous (this is stored under the 'class' feature as 'e' for edible, or 'p' for poisonous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions from Prior Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:21:55.356223Z",
     "start_time": "2020-01-21T03:21:55.339720Z"
    },
    "code_folding": [
     0,
     105
    ]
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes=None, normalize=False,cmap=None,\n",
    "                          title='Confusion Matrix',title_font={'size':14},\n",
    "                          annot_kws={'size':10,'weight':50}, \n",
    "                          axislabel_font={'size':14,'weight':70}, \n",
    "                          tick_font={'size':12,'weight':50},x_rot =45, y_rot=0,\n",
    "                         fig_kws={'figsize':(5,5)}):\n",
    "    \"\"\" Plots a confusion matrix of either a pre-calculated cm or a tuple of (y_true,y_pred) as cm.\n",
    "    \n",
    "    Args:\n",
    "        cm (array or tuple): Either a confusion amtrix from sklearn or (y_true,y_pred) tuple\n",
    "        classes (list, optional): Names of classes to use. Defaults to integers 0 to len(cm).\n",
    "        normalize (bool, optional): Annotate class-percentages instead of counts. Defaults to False.\n",
    "        cmap (cmap, optional): colormap to use Defaults to plt.get_cmap(\"Blues\").\n",
    "        title (str, optional): Plot title. Defaults to 'Confusion Matrix'.\n",
    "        title_font (dict, optional): fontdict for set_title. Defaults to {'size':14}.\n",
    "        annot_kws (dict, optional): kws for ax.Text annotations. Defaults to {'size':10,'weight':50}.\n",
    "        axislabel_font (dict, optional): fontdict for ylabel,xlabel. Defaults to {'size':14,'weight':70}.\n",
    "        tick_font (dict, optional): kws for plt.xticks/yticks. Defaults to {'size':12,'weight':50}.\n",
    "        x_rot (int, optional): Rotation of x-axis tick labels. Defaults to 45.\n",
    "        y_rot (int, optional): Rotation of y-axis tick labels.Defaults to 0.\n",
    "        fig_kws (dict, optional): kws for plt.subplots. Defaults to {}.\n",
    "    \n",
    "    Returns:\n",
    "        fig,ax: matplotlib Figure & Axes\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "    import sklearn.metrics as metrics\n",
    "    \n",
    "    ## If (y_true,y_pred) passed as cm\n",
    "    if isinstance(cm, tuple):\n",
    "        cm = metrics.confusion_matrix(*cm)\n",
    "        \n",
    "    ## if normalize:  normalize counts to class-accuracy\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "\n",
    "    \n",
    "    ## Setting & updating default kws\n",
    "    subplots_kws = {}\n",
    "    subplots_kws.update(fig_kws)\n",
    "    \n",
    "    ## Annotation kws\n",
    "    text_kws = dict(horizontalalignment=\"center\")\n",
    "    text_kws.update(annot_kws)    \n",
    "    \n",
    "    ## Axis Labels\n",
    "    axlabel_kws = dict(size=12, weight='bold')\n",
    "    axlabel_kws.update(axislabel_font)\n",
    "    \n",
    "    ## Tick Labels\n",
    "    ticklabel_kws = dict(size=10)\n",
    "    ticklabel_kws.update(tick_font)\n",
    "    \n",
    "\n",
    "    ## Define classes if not \n",
    "    if classes is None:\n",
    "        classes = list(range(len(cm)))\n",
    "        \n",
    "    ## Default cmap\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap(\"Blues\")\n",
    "\n",
    "\n",
    "\n",
    "    ## Create fig,ax and plot iamge\n",
    "    fig, ax = plt.subplots(**subplots_kws)\n",
    "    \n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.set_title(title,fontdict=title_font)\n",
    "\n",
    "    \n",
    "    ## Create Ticks\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    \n",
    "    plt.xticks(tick_marks, classes, rotation=x_rot,**ticklabel_kws)\n",
    "    plt.yticks(tick_marks, classes, rotation=y_rot,**ticklabel_kws)\n",
    "\n",
    "    ## Set annotation fmt and color threshold\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    \n",
    "    ## Add cm labels\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        # text_kws.update(color=color)\n",
    "        ax.text(j, i, format(cm[i, j], fmt),color=\"white\" if cm[i, j] > thresh else \"black\",fontdict=text_kws)\n",
    "                \n",
    "    ## Set axis labels\n",
    "    ax.set_ylabel('True Label',fontdict=axislabel_font)\n",
    "    ax.set_xlabel('Predicted Label',fontdict=axislabel_font)\n",
    "     \n",
    "    ## Add colorbar\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "    fig.colorbar(im,cax=cax)     \n",
    "\n",
    "#     plt.tight_layout()\n",
    "\n",
    "    return fig,ax\n",
    "\n",
    "\n",
    "\n",
    "def plot_auc_roc_curve(y_test, y_test_pred,figsize=(8,4)):\n",
    "    \"\"\" Takes y_test and y_test_pred from a ML model and uses sklearn roc_curve to plot the AUC-ROC curve.\"\"\"\n",
    "    from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    assert y_test.shape==y_test_pred.shape\n",
    "    auc = roc_auc_score(y_test, y_test_pred)#[:,1])\n",
    "\n",
    "    FPr, TPr, _  = roc_curve(y_test, y_test_pred)#[:,1])\n",
    "#     auc()\n",
    "    fig,ax=plt.subplots(figsize=figsize)\n",
    "    ax.plot(FPr, TPr,label=f\"AUC for Classifier:\\n{round(auc,2)}\" )\n",
    "\n",
    "    ax.plot([0, 1], [0, 1],  lw=2,linestyle='--')\n",
    "    ax.set_xlim([-0.01, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "#     plt.show()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:21:55.361502Z",
     "start_time": "2020-01-21T03:21:55.358300Z"
    }
   },
   "outputs": [],
   "source": [
    "len(df_ohe.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:21:55.366210Z",
     "start_time": "2020-01-21T03:21:55.362664Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:21:55.556715Z",
     "start_time": "2020-01-21T03:21:55.367918Z"
    }
   },
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, y_train)\n",
    "y_hat_test = tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:21:55.561098Z",
     "start_time": "2020-01-21T03:21:55.557998Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## WRITE WITH THE STUDENTS\n",
    "def plot_importance(tree,X,top_n=20,figsize=(10,10)):\n",
    "    \"\"\"Extracts feature importance from tree, plots the top_n and returns the full series.\"\"\"\n",
    "    df_importance = pd.Series(tree.feature_importances_,index=X.columns)\n",
    "    df_importance.sort_values(ascending=True).tail(top_n).plot(kind='barh',figsize=figsize)\n",
    "    return df_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:21:55.567893Z",
     "start_time": "2020-01-21T03:21:55.563940Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## WRITE WITH THE STUDENTS \n",
    "def evaluate_model(y_test,y_hat_test,tree):\n",
    "    \"\"\"Prints classification report and plots feature importance, confusion matrix, and roc curve.\"\"\"\n",
    "    print(metrics.classification_report(y_test,y_hat_test))\n",
    "    try:\n",
    "        df_important = plot_importance(tree,X_train)\n",
    "    except:\n",
    "        df_important = None\n",
    "\n",
    "    fig, ax = plot_confusion_matrix((y_test,y_hat_test),classes=[0,1],x_rot=0 ,\n",
    "                                    normalize=True)\n",
    "    plot_auc_roc_curve(y_test,y_hat_test);\n",
    "    return df_important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:21:56.432367Z",
     "start_time": "2020-01-21T03:21:55.569554Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_model(y_test,y_hat_test,tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:21:56.438376Z",
     "start_time": "2020-01-21T03:21:56.433685Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_tree(tree,feature_names=None,class_names=['0','1'],export_graphviz_kws={}):\n",
    "    \"\"\"Visualizes a sklearn tree using sklearn.tree.export_graphviz\"\"\"\n",
    "    from sklearn.tree import export_graphviz\n",
    "    from IPython.display import SVG\n",
    "    from graphviz import Source\n",
    "    from IPython.display import display\n",
    "    if feature_names is None:\n",
    "        feature_names=X_train.columns\n",
    "\n",
    "    tree_viz_kws =  dict(out_file=None, rotate=False, filled = True)\n",
    "    tree_viz_kws.update(export_graphviz_kws)\n",
    "\n",
    "    # tree.export_graphviz(dt) #if you wish to save the output to a dot file instead\n",
    "    graph = Source(export_graphviz(tree,feature_names=feature_names, class_names=class_names,**tree_viz_kws))\n",
    "    display(SVG(graph.pipe(format='svg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:22:39.595440Z",
     "start_time": "2020-01-21T03:21:56.440113Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize_tree(tree,X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:22:39.602206Z",
     "start_time": "2020-01-21T03:22:39.596615Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Timer():\n",
    "    def __init__(self, start=True,time_fmt='%m/%d/%y - %T'):\n",
    "        import tzlocal\n",
    "        import datetime as dt\n",
    "        \n",
    "        self.tz = tzlocal.get_localzone()\n",
    "        self.fmt= time_fmt\n",
    "        self._created = dt.datetime.now(tz=self.tz)\n",
    "        \n",
    "        if start:\n",
    "            self.start()\n",
    "            \n",
    "    def get_time(self):\n",
    "        import datetime as dt\n",
    "        return dt.datetime.now(tz=self.tz)\n",
    "\n",
    "        \n",
    "    def start(self,verbose=True):\n",
    "        self._laps_completed = 0\n",
    "        self.start = self.get_time()\n",
    "        if verbose: \n",
    "            print(f'[i] Timer started at {self.start.strftime(self.fmt)}')\n",
    "    \n",
    "    def stop(self, verbose=True):\n",
    "        self._laps_completed += 1\n",
    "        self.end = self.get_time()\n",
    "        self.elapsed = self.end -  self.start\n",
    "        if verbose: \n",
    "            print(f'[i] Timer stopped at {self.end.strftime(self.fmt)}')\n",
    "            print(f'  - Total Time: {self.elapsed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:25:18.410365Z",
     "start_time": "2020-01-21T03:22:39.603475Z"
    }
   },
   "outputs": [],
   "source": [
    "timer = Timer()\n",
    "grid = {'max_depth': [3,5,10,15],\n",
    "     'criterion': ['gini','entropy'],\n",
    "     'min_samples_split':[2,5,10],\n",
    "     'min_samples_leaf':[1,2,3,5,10],\n",
    "       'max_features': [3,5,10,20,30,40,50,60,70,80]}#10,20,50,len(X.columns)]}\n",
    "\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "\n",
    "# tree_cv = RandomizedSearchCV(dt_clf,param_distributions=grid,cv=3,verbose=True,n_iter=100)\n",
    "tree_cv = GridSearchCV(dt_clf,grid)     \n",
    "tree_cv.fit(X_train, y_train)\n",
    "\n",
    "timer.stop()\n",
    "print(tree_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:25:19.277165Z",
     "start_time": "2020-01-21T03:25:18.411526Z"
    }
   },
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(**tree_cv.best_params_)\n",
    "tree.fit(X_train,y_train)\n",
    "\n",
    "y_hat_test = tree.predict(X_test)\n",
    "y_hat_train = tree.predict(X_train)\n",
    "\n",
    "print(f\"[i] Training Data:\\n{metrics.classification_report(y_train,y_hat_train)}\",end='\\n\\n')\n",
    "# print(f\"[i] Test Data:\\n{metrics.classification_report(y_test,y_hat_test)}\")\n",
    "# print(metrics.confusion_matrix(y_test,y_hat_test))\n",
    "\n",
    "evaluate_model(y_test,y_hat_test,tree);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## You can also visualize your Decision Trees\n",
    "\n",
    "> Note: This requires installing graphviz which can be a painful installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:25:20.064336Z",
     "start_time": "2020-01-21T03:25:19.278415Z"
    }
   },
   "outputs": [],
   "source": [
    "visualize_tree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:25:20.067851Z",
     "start_time": "2020-01-21T03:25:20.065500Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Create the pipeline\n",
    "# pipetree = Pipeline([('enc',LabelEncoder()),\n",
    "#                      ('ohe',OneHotEncoder()),\n",
    "#                     ('dt',DecisionTreeClassifier())])\n",
    "\n",
    "# # pipe = Pipeline([('scl', MinMaxScaler()),\n",
    "# #                 ('pca', PCA(n_components=10)),\n",
    "# #                 ('svm', svm.SVC(random_state=123))])\n",
    "\n",
    "# # Create the grid parameter\n",
    "# grid = {'dt__max_depth': [3,5,10],\n",
    "#      'dt__criterion': ['gini','entropy'],\n",
    "#      'dt__min_samples_split':[2,5,10],\n",
    "#      'dt__min_samples_leaf':[1,2,3]}\n",
    "# @timeit\n",
    "# def timed_search(pipe=pipetree, grid=grid):\n",
    "#     randomsearch = RandomizedSearchCV(estimator=pipe,param_distributions=grid,verbose=1)\n",
    "\n",
    "#     randomsearch.fit(X_train, y_train)\n",
    "#     return randomsearch\n",
    "# # # Create the grid parameter\n",
    "# # grid = [{'svm__kernel': ['poly', 'sigmoid'],\n",
    "# #          'svm__C': [0.01, 1, 100],\n",
    "# #          'svm__degree0': [2,3,4,5],\n",
    "# #          'svm__gamma': [0.001, 0.01]}]\n",
    "\n",
    "# # # Create the grid, with \"pipe\" as the estimator\n",
    "# # gridsearch = GridSearchCV(estimator=pipe,\n",
    "# #                   param_grid=grid,\n",
    "# #                   scoring='accuracy',\n",
    "# #                   cv=3)\n",
    "\n",
    "# # Fit using grid search\n",
    "# # gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A single decision tree will often overfit your training data. There are steps one can take to help with this, like limiting the \"depth\" of the nodes. But it's often better to do something else: Plant another tree!\n",
    "\n",
    "Of course, if a second tree is going to be of any value, it has to be *different from* the first. Here's a good algorithm for achieving that:\n",
    "\n",
    "## Fitting a Set of Bagged Decision Trees\n",
    "\n",
    "### Bagging Algorithm\n",
    "\n",
    "Take a sample of your X_train and fit a decision tree to it. <br/>\n",
    "Replace the first batch of data and repeat. <br/>\n",
    "When you've got as many trees as you like, make use of all your individual trees' predictions to come up with some holistic prediction. (Most obviously, we could take the average of our predictions, but there are other methods we might try.)\n",
    "\n",
    "<br/>\n",
    "\n",
    "Because we're resampling our data with replacement, we're *bootstrapping*. <br/>\n",
    "Because we're making use of our many samples' predictions, we're *aggregating*. <br/>\n",
    "Because we're bootstrapping and aggregating all in the same algorithm, we're *bagging*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:25:20.071768Z",
     "start_time": "2020-01-21T03:25:20.069180Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier,BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:25:29.905949Z",
     "start_time": "2020-01-21T03:25:20.072748Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "bag = BaggingClassifier(n_estimators=100)\n",
    "bag.fit(X_train, y_train)\n",
    "print(bag.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:25:30.502301Z",
     "start_time": "2020-01-21T03:25:29.907216Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "y_hat_test = bag.predict(X_test)\n",
    "\n",
    "evaluate_model(y_test,y_hat_test,bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "That's a significant improvement in accuracy! Let's see if we can do even better.\n",
    "\n",
    "## Fitting a Random Forest\n",
    "\n",
    "### Random Forest Algorithm\n",
    "\n",
    "Let's add an extra layer of randomization: Instead of using *all* the features of my model to optimize a branch at each node, I'll just choose a subset of my features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:25:33.169129Z",
     "start_time": "2020-01-21T03:25:30.503797Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train, y_train)\n",
    "print(rf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:25:34.088825Z",
     "start_time": "2020-01-21T03:25:33.170794Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "y_hat_test = rf.predict(X_test)\n",
    "evaluate_model(y_test,y_hat_test,rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fitting a Stand of Extremely Randomized Trees\n",
    "\n",
    "### Extra Trees Algorithm\n",
    "\n",
    "Sometimes we might want even one more bit of randomization. Instead of always choosing the *optimal* branching path, we might just choose a branching path at random. If we're doing that, then we've got extremely randomized trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:25:38.372651Z",
     "start_time": "2020-01-21T03:25:34.090135Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "et = ExtraTreesClassifier(n_estimators=100)\n",
    "et.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:25:38.659872Z",
     "start_time": "2020-01-21T03:25:38.374088Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "et.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gridsearching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:25:38.663850Z",
     "start_time": "2020-01-21T03:25:38.661264Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [30, 100, 300],\n",
    "    'min_samples_split': [2, 4, 6],\n",
    "    'min_samples_leaf': [2, 4, 6]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:25:38.667577Z",
     "start_time": "2020-01-21T03:25:38.665153Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(et, param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:32:12.226076Z",
     "start_time": "2020-01-21T03:25:38.668829Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:32:12.727435Z",
     "start_time": "2020-01-21T03:32:12.227352Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:32:12.731373Z",
     "start_time": "2020-01-21T03:32:12.728516Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:32:24.121661Z",
     "start_time": "2020-01-21T03:32:12.732683Z"
    }
   },
   "outputs": [],
   "source": [
    "et = ExtraTreesClassifier(**gs.best_params_)\n",
    "et.fit(X_train,y_train)\n",
    "y_hat_test = et.predict(X_test)\n",
    "\n",
    "evaluate_model(y_test,y_hat_test,et)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:32:29.101534Z",
     "start_time": "2020-01-21T03:32:24.122867Z"
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier,XGBRFClassifier\n",
    "\n",
    "xgb_rf = XGBRFClassifier()\n",
    "xgb_rf.fit(X_train, y_train)\n",
    "print(xgb_rf.score(X_test,y_test))\n",
    "\n",
    "y_hat_test = xgb_rf.predict(X_test)\n",
    "\n",
    "evaluate_model(y_test,y_hat_test,xgb_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:32:29.105369Z",
     "start_time": "2020-01-21T03:32:29.102534Z"
    }
   },
   "outputs": [],
   "source": [
    "xgbrf_grid = {'colsample_bynode': 0.8, 'learning_rate': 1,\n",
    "              'max_depth': 5, 'num_parallel_tree': 100, \n",
    "              'objective': 'binary:logistic', 'subsample': 0.8}\n",
    "\n",
    "xrf_clf = XGBRFClassifier(**xgbrf_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:32:36.082347Z",
     "start_time": "2020-01-21T03:32:29.107034Z"
    }
   },
   "outputs": [],
   "source": [
    "xrf_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:32:36.870669Z",
     "start_time": "2020-01-21T03:32:36.083486Z"
    }
   },
   "outputs": [],
   "source": [
    "y_hat_test = xrf_clf.predict(X_test)\n",
    "\n",
    "evaluate_model(y_test,y_hat_test,xrf_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
