{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GKw2nWdMDKR6"
   },
   "source": [
    "# Network Regularlization & Normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lbzweQKTDPTl"
   },
   "source": [
    "## Overview - Regularization\n",
    "- Bias vs variance trade-off\n",
    "- Using test, train, and vali splits. \n",
    "- Prevent overfitting by adding regularization methods (L1, L2, dropout)\n",
    "- Optimizing and training time reduction by normalizing inputs\n",
    "    - Normalizing inputs can drasticaly decrease computation time, and prevent vanishing/exploding graidents. \n",
    "    \n",
    "### Hyperparameters to Tune\n",
    "- Number of hidden units\n",
    "- Number of layers\n",
    "- Learning rate ( $\\alpha$)\n",
    "- Activation function\n",
    "\n",
    "### Training, Validation, and Test Sets\n",
    "- The fact that there are so many hyperparameters to tune calls for a formalized and unbiased approach to testing/training sets.\n",
    "- We will use 3 sets when running, selecting, and validating a model:\n",
    "    - Training set: for training the alogrithm\n",
    "    - Validation set: to decide which model will be the final one after parameter tuning\n",
    "    - Testing set: after choosing final  the final model, use the test set for an inbiased estimate of performance. \n",
    "- Set sizes:\n",
    "    - With big data, your dev and test sets don't necessarily need to be 20-30% of all the data. \n",
    "    - You can choose test and hold-out sets that are of size 1-5%. \n",
    "        - eg. 96% train, 2% hold-out, 2% test set. \n",
    "    - It is **VERY IMPORTANT** to make sure holdout and test sample come from the same distribution: eg. same resolution of santa pictures. \n",
    "    \n",
    "### Bias vs Variance \n",
    "- A model with high bias may result in underfitting.\n",
    "    - <img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-42-02-tuning-neural-networks-with-regularization-online-ds-ft-021119/master/figures/underfitting.png\" width=200>\n",
    "- A model with high variance may result in overfitting. \n",
    "    - <img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-42-02-tuning-neural-networks-with-regularization-online-ds-ft-021119/master/figures/overfitting.png\" width=200>\n",
    "\n",
    "- In deep learning, there is less of a bias-variance trad-off vs simpler models. \n",
    "\n",
    "**Rules of thumb re: bias/variance trade-off:**\n",
    "\n",
    "| High Bias? (training performance) | high variance? (validation performance)  |\n",
    "|---------------|-------------|\n",
    "| Use a bigger network|    More data     |\n",
    "| Train longer | Regularization   |\n",
    "| Look for other existing NN architextures |Look for other existing NN architextures |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BlPs5Wd8GrNg"
   },
   "source": [
    "### L1 & L2 Regularlization\n",
    "- These methods of regularizaiton do so by penalizing coefficients(regression) or weights(neural networks),\n",
    "    - L1 & L2 exist in regression models as well. There, L1='Lasso Regressions' , L2='Ridge regression'\n",
    "\n",
    "- **L1 & L2 regularization add a term to the cost function.**\n",
    "\n",
    "$$Cost function = Loss (say, binary cross entropy) + Regularization term$$\n",
    "\n",
    "$$ J (w^{[1]},b^{[1]},...,w^{[L]},b^{[L]}) = \\dfrac{1}{m} \\sum^m_{i=1}\\mathcal{L}(\\hat y^{(i)}, y^{(i)})+ \\dfrac{\\lambda}{2m}\\sum^L_{l=1}||w^{[l]}||^2$$\n",
    "\n",
    "    - where $\\lambda$ is the regularization parameter. \n",
    "    - The difference between  L1 vs L2 is that L1 is just the sum of the weights whereas L2 is the sum of the _square_of the weights.  \n",
    "\n",
    "- **L1 Regularization:**\n",
    "    $$ Cost function = Loss + \\frac{\\lambda}{2m} * \\sum ||w||$$\n",
    "    - Uses the absolute value of weights and may reduce the weights down to 0. \n",
    "    \n",
    "        \n",
    "- **L2 Regularization:**:\n",
    "    $$ Cost function = Loss + \\frac{\\lambda}{2m} * \\sum ||w||^2$$\n",
    "    - Also known as weight decay, as it forces weights to decay towards zero, but never exactly 0.. \n",
    "    \n",
    "-  Regularization term $||w^{[l]}||^2 _F$  is  A.K.A. The Frobenius Norm\n",
    "    - $||w^{[l]}||^2 = \\sum^{n^{[l-1]}}_{i=1} \\sum^{n^{[l]}}_{j=1} (w_{ij}^{[l]})^2$\n",
    "\n",
    "    \n",
    "- **CHOOSING L1 OR L2:**\n",
    "    - L1 is very useful when trying to compress a model. (since weights can decrease to 0)\n",
    "    - L2 is generally preferred otherwise.\n",
    "    \n",
    "- **USING L1/L2 IN KERAS:**\n",
    "    - Add a kernel_regulaizer to a  layer.\n",
    "```python \n",
    "from keras import regularizers\n",
    "model.add(Dense(64, input_dim=64, kernel_regularizer=regularizers.l2(0.01))\n",
    "```\n",
    "    - here 0.01 = $\\lambda$\n",
    "\n",
    "### Dropout Regularization\n",
    "- Uses a specified probablity to random leave out a node from a ---epoch?\n",
    "\n",
    "\n",
    "- **USING DROPOUT IN KERAS:**\n",
    "    - Dropout layers are located in keras.layers.core \n",
    "    - Specify probably of being exlcuded/dropped out.\n",
    "```python\n",
    "from keras.layers.core import Dropout\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(output_dim=hidden1_num_units, input_dim=input_num_units, activation='relu'))\n",
    "model.add(layers.core.Dropout(Dropout(0.25))                              \n",
    "```\n",
    "\n",
    "### Data Augmentation (not covered in class)\n",
    "- Simplest way to reduce overfitting is to increase the size of the training data.\n",
    "- Difficult to do with large datasets, but can be implemented with images as shown below:\n",
    "- **For augmenting image data:**\n",
    "    - Can alter the images already present in the training data by shifting, shearing, scaling, rotating.<br><br> <img src =\"https://www.dropbox.com/s/9i1hl3quwo294jr/data_augmentation_example.png?raw=1\" width=300>\n",
    "    - This usually provides a big leap in improving the accuracy of the model. It can be considered as a mandatory trick in order to improve our predictions.\n",
    "\n",
    "- **In Keras:**\n",
    "    - `ImageDataGenerator` contains several augmentations available.\n",
    "    - Example below:\n",
    "    \n",
    "```python\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(horizontal flip=True)\n",
    "datagen.fit(train)\n",
    "```\n",
    "### Early Stopping (not covered in class)\n",
    "- Monitor performance for decrease or plateau in performance, terminate process when given criteria is reached.\n",
    "\n",
    "- **In Keras:**\n",
    "    - Can be applied using the [callbacks function](https://keras.io/callbacks/)\n",
    "```python    \n",
    "from keras.callbacks import EarlyStopping\n",
    "EarlyStopping(monitor='val_err', patience=5)\n",
    "```\n",
    "    - 'Monitor' denotes quanitity to check\n",
    "    - 'val_err' denotes validation error\n",
    "    - 'pateience' denotes # of epochs without improvement before stopping.\n",
    "        - Be careful, as sometimes models _will_ continue to improve after a stagnant period\n",
    "\n",
    "### Reference Links I found:\n",
    "- https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/\n",
    "- http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y0QuBXKwRlbc"
   },
   "source": [
    "## Overview - Network Optimization\n",
    "### Normalization\n",
    "- Normalizing to a consistent scale (typically 0 to 1) improves performance, but also ensures the process will converge to a stable solution. \n",
    "\n",
    "- Methods:\n",
    "    - Z-Score (subtracting mean, normalize by standard deviation)\n",
    "    \n",
    "#### Reference Links\n",
    "- https://www.coursera.org/lecture/deep-neural-network/normalizing-inputs-lXv6U\n",
    "\n",
    "### Changing Initial Parameters\n",
    "- The more input features into layer $l$, the smaller we want each weight $w_i$ to be.\n",
    "- Rule of thumb:\n",
    "    - $Var(w_i) = 1/n$ or $2/n$\n",
    "- A common initilization strategy for the relu activation functions is:\n",
    "\n",
    "    * $w^{[l]}$ `= np.random.randn(shape)*np.sqrt(2/n_(l-1))`\n",
    "    \n",
    "## Optimization:\n",
    "Alternatives to gradient descent that do not oscillate as much as g.d.:\n",
    "### Gradient Descent with Momentum:\n",
    "- Comutes an exponentially weighted average of the gradients to use.\n",
    "    - will dampen oscillations and improve performance.\n",
    "- How to:\n",
    "    -  Calculate current batch's moving averages for the derivatives of $W$ and$b$\n",
    "        - Compute $V_{dw} = \\beta V_{dw} + (1-\\beta)dW$\n",
    "        - $V_{db} = \\beta V_{db} + (1-\\beta)db$ \n",
    "        - So updated terms become\n",
    "            - $W:= W- \\alpha Vdw$\n",
    "            -$b:= b- \\alpha Vdb$\n",
    "    -  Generally, $\\beta=0.9$ is a good hyperparameter value.\n",
    "    \n",
    "### RMSprop\n",
    "- \"Root mean square\" prop\n",
    "- Slow down learning in one direction and speed it up in another.\n",
    "    - In the direction where we want to learn fast, the corresponding S will be small, so dividing by a small number. \n",
    "    - In the direction where we will want to learn slow, the corresponding S will be relatively large, and updates will be smaller. \n",
    "- How to:\n",
    "    - On each iteration, use exponentially weighted average again:\n",
    "        - exponentially weighted average of the squares of the derivatives\n",
    "        - $S_{dw} = \\beta S_{dw} + (1-\\beta)dW^2$\n",
    "        - $S_{db} = \\beta S_{dw} + (1-\\beta)db^2$\n",
    "        - So that:\n",
    "            - $W:= W- \\alpha \\dfrac{dw}{\\sqrt{S_{dw}}}$\n",
    "            - $b:= b- \\alpha \\dfrac{db}{\\sqrt{S_{db}}}$\n",
    "    - Often, add small $\\epsilon$ in the denominator to make sure that you don't end up dividing by 0.\n",
    "\n",
    "\n",
    "### Adam Optimization Algorithm\n",
    "- Adaptive Moment Estimation - essentially combines both methods above.\n",
    "- Works very well in most situations.\n",
    "- How to: \n",
    "    - Initialize: $V_{dw}=0, S_{dw}=0, V_{db}=0, S_{db}=0$.\n",
    "    - For each teration: compute $dW, db$ using the current mini-batch.\n",
    "        -  $V_{dw} = \\beta_1 V_{dw} + (1-\\beta_1)dW$, $V_{db} = \\beta_1 V_{db} + (1-\\beta_1)db$ \n",
    "        -  $S_{dw} = \\beta_2 S_{dw} + (1-\\beta_2)dW^2$, $S_{db} = \\beta_2 S_{db} + (1-\\beta_2)db^2$ \n",
    "        \n",
    "- As with  momentum and then RMSprop. We need to perform a correction! This is sometimes also done in RSMprop, but definitely here too.\n",
    "    - $V^{corr}_{dw}= \\dfrac{V_{dw}}{1-\\beta_1^t}$, $V^{corr}_{db}= \\dfrac{V_{db}}{1-\\beta_1^t}$\n",
    "\n",
    "    - $S^{corr}_{dw}= \\dfrac{S_{dw}}{1-\\beta_2^t}$, $S^{corr}_{db}= \\dfrac{S_{db}}{1-\\beta_2^t}$\n",
    "\n",
    "    - $W:= W- \\alpha \\dfrac{V^{corr}_{dw}}{\\sqrt{S^{corr}_{dw}+\\epsilon}}$ and\n",
    "\n",
    "    - $b:= b- \\alpha \\dfrac{V^{corr}_{db}}{\\sqrt{S^{corr}_{db}+\\epsilon}}$ \n",
    "\n",
    "\n",
    "### Learning Rate Decay\n",
    "- Learning rate decreases across epochs.\n",
    "    - $\\alpha = \\dfrac{1}{1+\\text{decay_rate * epoch_nb}}* \\alpha_0$\n",
    "\n",
    "- other methods:\n",
    "    - $\\alpha = 0.97 ^{\\text{epoch_nb}}* \\alpha_0$ (or exponential decay)<br>OR:\n",
    "    - $\\alpha = \\dfrac{k}{\\sqrt{\\text{epoch_nb}}}* \\alpha_0$<br> OR:\n",
    "    - Manual decay.\n",
    "    \n",
    "    \n",
    "    \n",
    "### HYPERPARAMETER TUNING:\n",
    "Most important:\n",
    "- $\\alpha$\n",
    "\n",
    "Important next:\n",
    "- $\\beta$ (momentum)\n",
    "- Number of hidden units\n",
    "- mini-batch-size\n",
    "\n",
    "Finally:\n",
    "- Number of layers\n",
    "- Learning rate decay\n",
    "\n",
    "Almost never tuned:\n",
    "- $\\beta_1$, $\\beta_2$, $\\epsilon$ (Adam)\n",
    "\n",
    "- Tip: Don't use a grid, because hard to say in advance which hyperparameters will be important.\n",
    "\n",
    "\n",
    "### OPTIMIZAITON REFS:\n",
    "- https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "- https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
    "- https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/\n",
    "- https://stackoverflow.com/questions/37232782/nan-loss-when-training-regression-network https://www.springboard.com/blog/free-public-data-sets-data-science-project/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "My Flatiron Bootcamp Notes - Mod 4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "325px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
