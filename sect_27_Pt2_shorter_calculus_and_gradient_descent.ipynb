{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 27- Pt2: Calculus, Cost Functions, and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- online-ds-ft-100719\n",
    "- 12/16/19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Today's Agenda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Group poll (5 min): which lessons to review this week? Mod 4 project-specific first?**\n",
    "\n",
    "    > **Q: Skip over non-project related lessons and cover all of mod 4 project-related material BEFORE break?**\n",
    "    > A: Mod 4 project stuff\n",
    "    \n",
    "    - **Mod 4 Project Related Material:**\n",
    "        - Sect 31: Working with Time Series\n",
    "        - Sect 32: Time Series Modeling\n",
    "        \n",
    "    - **Non-Project Material for After Break:**\n",
    "        - Sect 28: Extensions to Linear Models\n",
    "        - Sect 29: Intro to Logistic Regression\n",
    "        - Sect 30: In-Depth Logistic Regression\n",
    "    \n",
    "    \n",
    "- **Review where we left off last week (5 -10 min):**\n",
    "    - Linear Algebra\n",
    "    - Cost Functions & Gradient Descent\n",
    "\n",
    "\n",
    "- **We will be watching 2 blocks of  Udemy lessons as a group and discussing (~35 min)**\n",
    "    - Udemy Course:\n",
    "    [Data Science Complete Bootcamp](https://www.udemy.com/share/101W9cAEYbdVdWRXQ=/)\n",
    "    - Block 1: From Linear Regression to Linear Models (9 min + discussion)\n",
    "    - Block 2: Gradient Descent (15 min + discussion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview Udemy Lessons to Watch\n",
    "\n",
    "[Data Science Complete Bootcamp](https://www.udemy.com/share/101W9cAEYbdVdWRXQ=/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Watch Block 1: From Linear Regression to Linear Models** (~9 mins of video + discussion =10-15 min)\n",
    "    - Section 42: \n",
    "        - 288: The Linear Model (Algebraic) (3 min)\n",
    "        - 289: The Linear Model with Multiple Inputs (2 min)\n",
    "        - 290: The Linear Model with Multiple Inputs & Outputs (4 min)\n",
    "    - Discuss & Summarize block 1 (~5 min)\n",
    "    \n",
    "\n",
    "- **Watch Block 2: Gradient Descent** (~15 mins of video + discussion =  20-25 min )\n",
    "    - Section 42: \n",
    "        - 295: Optimization Algorithm: 1-Parameter Gradient Descent (7 min: ONLY 0m00s ~ 3m30s)\n",
    "                \n",
    "        - **296: Optimization Algorithm: n-Parameter Gradient Descent (6 min)**\n",
    "        \n",
    "    - Section 48:\n",
    "        - **329: Stochastic Gradient Descent (3 min)**\n",
    "        - **330: Problems with Gradient Descent (2 min)**\n",
    "        \n",
    "    - Discuss & summarize block 2 (~10 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     \n",
    "- **Appendix Block: Watch if there are questions about improving gradient descent.**\n",
    "    - 331 Momentum (4 min)\n",
    "    - 332 Learning Rate Schedules (4 min)\n",
    "    - 334 Adaptive Learning Rate Schedules (4 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block 1: From Linear Regression to Linear Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Objectives:**\n",
    "- Gain an intuitive understand linear models by starting with linear regression and extending to n-dimensional linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watch Block 1 (~9 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Data Science Complete Bootcamp](https://www.udemy.com/share/101W9cAEYbdVdWRXQ=/)\n",
    "    - Section 42: \n",
    "        - **288: The Linear Model (Algebraic)** (3 min)\n",
    "        - **289: The Linear Model with Multiple Inputs** (2 min)\n",
    "        - **290: The Linear Model with Multiple Inputs & Outputs**(4 min)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize Block 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can start to see how linear algebra is important/pertinent to machine learnings, even if we are not directly doing the linear regression.\n",
    "\n",
    "- We started seeing multiple slopes and intercepts in a linear model, which are instead referred to as weights and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference: Linear Regression and Linear Model Equations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**in Linear Regression, we predict $y$ using 2 parameters, m (slope) + b(intercept/constant):**\n",
    "\n",
    "$$ \\large y = mx+b $$\n",
    "where: \n",
    "    \n",
    "- $x$ = input data for modeling\n",
    "- $y$ = model] predictions\n",
    "- $m$ = slope\n",
    "- $b$ = intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In Linear Model Formulas, terminology/notation changes:**\n",
    "\n",
    "- slopes $(m)$ becomes **weights ($w$)**\n",
    "- constants $b$ becomes **biases ($b$)**\n",
    "\n",
    "$$ \\large xw+b = y $$\n",
    "\n",
    "\n",
    "- $x$ = input data for modeling\n",
    "- $y$ = model] predictions\n",
    "- $w$ is the weight (slope)\n",
    "- $b$ is the bias (constant)\n",
    "</div>\n",
    "</div>\n",
    " x_i w+b = y_i -t? $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lesson Screencaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear Algebra with Multiple Inputs\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/fsds_100719_cohort_notes/master/images/linear_model_multi_inputs.png\" width=40%>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/fsds_100719_cohort_notes/master/images/linear_model_multi_inputs_arrows.png\" width=40%>\n",
    "\n",
    "- Linear Algebra with Multiple Inputs and Outputs\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/fsds_100719_cohort_notes/master/images/linear _model_multi_inputs_and_outputs.png\" width=40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block 2: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Objectives:**\n",
    "- Understand how gradient descent works\n",
    "- Gain an intuitive understanding of G.D. via several analogies\n",
    "- Review RSS with Linear Regression to extend into Cost Function Gradients\n",
    "\n",
    "\n",
    "**Decide as a group how much to review before block of video.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Review Derivatives & Gradient Descent Analogies\n",
    "- Watch video block 2:\n",
    "    - [Data Science Complete Bootcamp](https://www.udemy.com/share/101W9cAEYbdVdWRXQ=/)\n",
    "    - Section 42: \n",
    "\n",
    "        - 295: Optimization Algorithm: 1-Parameter Gradient Descent <br>   (7 min; only watch0m00s ~ 3m30s)\n",
    "\n",
    "        - **296: Optimization Algorithm: n-Parameter Gradient Descent (6 min)**\n",
    "    - Section 48:\n",
    "        - **329: Stochastic Gradient Descent (3 min)**\n",
    "        - **330: Problems with Gradient Descent (2 min)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watch Block 2: Gradient Descent (~15 min)\n",
    "\n",
    "- Watch video block 2:\n",
    "    - [Data Science Complete Bootcamp](https://www.udemy.com/share/101W9cAEYbdVdWRXQ=/)\n",
    "    - Section 42: \n",
    "\n",
    "        - 295: Optimization Algorithm: 1-Parameter Gradient Descent <br>   (7 min; only watch0m00s ~ 3m30s)\n",
    "\n",
    "        - **296: Optimization Algorithm: n-Parameter Gradient Descent (6 min)**\n",
    "    - Section 48:\n",
    "        - **329: Stochastic Gradient Descent (3 min)**\n",
    "        - **330: Problems with Gradient Descent (2 min)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize Block 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Become comfortable loss =  cost = error\n",
    "    - \n",
    "- The weights and biases are updated independent\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing Gradient Descent & Cost Funcions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Functions: RSS & Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear Regression Model Equation:\n",
    "$$y=mx+b$$\n",
    "\n",
    "- Cost Function: RSS\n",
    "\n",
    " So in the graph directly below, $x_i$ and $y_i$  would be our points representing a movie's budget and revenue.  Meanwhile, $mx_i + b $ is our predicted $y$ value for a given $x$ value, of a budget. \n",
    "\n",
    "And RSS takes the difference between $mx_i + b$, the $y_i$ value our regression line (the red line below) predicts, and our actual $y$, and sum up these squares for each piece of data in our dataset.  That is the residual sum of squares.\n",
    "\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "RSS &= \\sum_{i=1}^n(actual - expected)^2 \\\\\n",
    "&= \\sum_{i=1}^n(y_i - \\hat{y})^2 \\\\\n",
    "&= \\sum_{i=1}^n(y_i - (mx_i + b))^2\n",
    "\\end{align}\n",
    "$$ \n",
    "\n",
    "for all $x$ and $y$ values of our dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- VISUAL OF RSS derivative going to 0 at minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivatives with Loss Functions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-gradient-descent-step-sizes-online-ds-ft-100719/master/images/snowboard.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivatives (reworded to match new concepts)\n",
    "\n",
    "The derivative of $f$ with respect to $x$: $f'(x)$  is:\n",
    "- The instantaneous rate of change in the output of $f(x)$, with respect to changes in the values of $x$.\n",
    "\n",
    "$$\\large f'(x) = \\dfrac{f(x + \\Delta x) - f(x)}{\\Delta x}$$\n",
    "\n",
    "* $f(x)$ is output of our model $f$ for a given $x$ value.\n",
    "\n",
    "* $f'(x)$ is the derivative (rate of change) of $f(x)$'s output  \n",
    "\n",
    "\n",
    "* $\\Delta x$ is the size of the step between $x_i$ and $x_{i+1}$ values and controls the rate at which we will retest and update our slope/derivative\n",
    "\n",
    "\n",
    "* $f(x + \\Delta x)$ the output of $f$ for the updated output when $x=(x +\\Delta x$)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent Upside Down: Mountain Climbing\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-students/dsc-the-gradient-in-gradient-descent-online-ds-ft-100719/master/images/Denali.jpg\">\n",
    "\n",
    "- Our function $f(x,y)$ represents the movement of the climbers towards the summit.\n",
    "\n",
    "The direction of greatest ascent for a function,  $\\nabla f(x, y)$, is the direction which is a proportion of $\\frac{df}{dy}$ steps in the $y$ direction and $\\frac{df}{dx}$ in the $x$ direction.  So, for example, if $\\frac{df}{dy}$ = 5 and $\\frac{df}{dx}$ = 1, the direction of gradient ascent is five times more in the $y$ direction than the $x$ direction.  And this seems to be the path, more or less that our climbers are taking - some combination of $x$ and $y$, but tilted more towards the $y$ direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent - Standing on a Rock\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-gradient-descent-in-3d-online-ds-ft-100719/master/images/traveller-stepping.jpg\" width=50%>\n",
    "\n",
    "> So how does this approach of shifting back and forth translate mathematically?  It means we determine the slope in one dimension, then the other. Then, we move where that slope is steepest downwards.  This moves us towards our minimum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src='images/gradientdescent.png' width=60%/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent with RSS\n",
    "\n",
    "\n",
    "- Cost Function $J$ (RSS):\n",
    "\n",
    "$$ \\large RSS =  \\sum_{i=1}^n(y_i - \\hat{y})^2 $$ \n",
    "\n",
    "$$ \\large J(m, b) = \\sum_{i=1}^{n}(y_i - \\hat{y})^2 $$\n",
    "\n",
    "- we know $\\hat{y} = mx + b$, so: \n",
    "\n",
    "$$ J(m, b) = \\sum_{i=1}^{n}(y_i - (mx_i + b))^2 $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partial Derivatives of RSS\n",
    "As we know, the gradient of a function is simply the partial derivatives with respect to each of the variables, so:\n",
    "\n",
    "$$ \\large \\nabla J(m, b) = \\frac{\\delta J}{\\delta m}, \\frac{\\delta J}{\\delta b}$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\delta J}{\\delta m}J(m, b) & = \\boldsymbol{\\frac{\\delta J}{\\delta m}}(y - (mx + b))^2  &&\\text{partial derivative with respect to} \\textbf{ m}\\\\\n",
    "\\\\\n",
    "\\frac{\\delta J}{\\delta b}J(m, b) & = \\boldsymbol{\\frac{\\delta J}{\\delta b}}(y - (mx + b))^2  &&\\text{partial derivative with respect to} \\textbf{ b}\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update Rule:\n",
    "\n",
    "In calculating the partial derivatives of our function $J(m, b) = \\sum_{i=1}^{n}(y_i - (mx_i + b))^2$, **we won't change the result if we ignore the summation until the very end**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\delta J}{\\delta m}J(m, b) & = \\boldsymbol{\\frac{\\delta J}{\\delta m}}(y - (mx + b))^2  &&\\text{partial derivative with respect to} \\textbf{ m}\\\\\n",
    "\\\\\n",
    "\\frac{\\delta J}{\\delta b}J(m, b) & = \\boldsymbol{\\frac{\\delta J}{\\delta b}}(y - (mx + b))^2  &&\\text{partial derivative with respect to} \\textbf{ b}\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalized Gradient Descent  Math Notation\n",
    "\n",
    "- In the below equations: \n",
    "     - x and x_i represent whatever paramater is being tested and updated. \n",
    "     \n",
    " \n",
    "- Multi-dimensional version of a derivative \n",
    "$$-\\nabla = \\sum_i \\dfrac{\\partial}{\\partial x_i}$$\n",
    "\n",
    "$$ x_{i+1} = x_i - \\eta * f'(x_i)$$\n",
    "\n",
    "\n",
    "- When minimum is reached, $ \\eta *f'(x_i) $ becomes 0. \n",
    "$$ x_{i+1} = x_i - 0 $$\n",
    "\n",
    "- [ADD PIC]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REMAINING \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX: NOTES TO MAYBE ADD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost/Loss/Error Functions\n",
    "\n",
    "- **Multiple notations/names:**\n",
    "\n",
    "    - Loss Function: $L(y,t)$\n",
    "\n",
    "    - Cost Function: $C(y,t)$\n",
    "\n",
    "    - Error: $E(y,t)$\n",
    "    \n",
    "- **Cost Functions We Know**\n",
    "    - RSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U fsds_100719\n",
    "# from fsds_100719.imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Updating Our Guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "> Now that we are applying gradient descent to our cost curve $J(m, b)$, the technique should answer how much to move the $m$ variable and the $b$ variable to produce the greatest decrease in cost, or RSS. In other words, when altering our regression line, we want to know how much of this change should be derived from a move in the slope versus how much should be derived from a change in the y-intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "$$ \\nabla J(m, b) = \\frac{\\delta J}{\\delta m}, \\frac{\\delta J}{\\delta b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\delta J}{\\delta m}J(m, b) & = \\boldsymbol{\\frac{\\delta J}{\\delta m}}(y - (mx + b))^2  &&\\text{partial derivative with respect to} \\textbf{ m}\\\\\n",
    "\\\\\n",
    "\\frac{\\delta J}{\\delta b}J(m, b) & = \\boldsymbol{\\frac{\\delta J}{\\delta b}}(y - (mx + b))^2  &&\\text{partial derivative with respect to} \\textbf{ b}\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$ \\frac{dJ}{dm}J(m,b) = -2\\sum_{i = 1}^n x_i(y_i - (mx_i + b)) = -2\\sum_{i = 1}^n x_i*\\epsilon_i$\n",
    "$ \\frac{dJ}{db}J(m,b) = -2\\sum_{i = 1}^n(y_i - (mx_i + b)) = -2\\sum_{i = 1}^n \\epsilon_i $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expanding Gradient Descent for Higher Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Grad function $-\\nabla = \\sum_i \\dfrac{\\partial}{\\partial x_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$ \\nabla J(m, b) = \\frac{\\delta J}{\\delta m}, \\frac{\\delta J}{\\delta b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\delta J}{\\delta m}J(m, b) & = \\boldsymbol{\\frac{\\delta J}{\\delta m}}(y - (mx + b))^2  &&\\text{partial derivative with respect to} \\textbf{ m}\\\\\n",
    "\\\\\n",
    "\\frac{\\delta J}{\\delta b}J(m, b) & = \\boldsymbol{\\frac{\\delta J}{\\delta b}}(y - (mx + b))^2  &&\\text{partial derivative with respect to} \\textbf{ b}\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lesson adapted from Jeff Herman: https://github.com/sik-flow/Study_Groups/blob/master/Gradient_Descent.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{dJ}{dm}J(m,b) = -2\\sum_{i = 1}^n x_i(y_i - (mx_i + b)) = -2\\sum_{i = 1}^n x_i*\\epsilon_i$\n",
    "$ \\frac{dJ}{db}J(m,b) = -2\\sum_{i = 1}^n(y_i - (mx_i + b)) = -2\\sum_{i = 1}^n \\epsilon_i $"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Walkthrough_Gradient_Descent.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "453.6px",
    "left": "0px",
    "top": "110.833px",
    "width": "230.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
