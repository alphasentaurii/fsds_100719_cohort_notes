{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 43: Foundations of Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Discuss word Embeddings and their advantages\n",
    "- Training Word2Vec models\n",
    "- Using pretrained word embeddings\n",
    "\n",
    "\n",
    "- Create a Classification Model for true-trump (\"Twitter for Android\") vs trump-staffer(\"Twitter for iPhone - from period of time when android was still in use)\n",
    "\n",
    "    - Use lesson's W2Vec class in Sci-kit learn models\n",
    "    - Use LSTMs\n",
    "    - Use RNN/GRUs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Compare:\n",
    "    1.  Mean embeddings vs count/tfidf data with scikit learn.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-word-embeddings-online-ds-ft-100719/master/images/embeddings.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert words into a vector space\n",
    "    + Mathematical object\n",
    "- It's all about closeness\n",
    "    + Distributional Hypothesis: https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-word-embeddings-online-ds-ft-100719/master/images/vectors.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kaggle Tutorial:  https://www.kaggle.com/learn/embeddings\n",
    "- Google Embedding Crash Course: https://developers.google.com/machine-learning/crash-course/embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-using-word2vec-online-ds-ft-100719/master/images/training_data.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train the MLP to find the best weights (context) to map word-to-word\n",
    "- But since words close to another usually contain context, we're _really_ teaching it context in those weights\n",
    "- Gut check: similar contexted words can be exchanged\n",
    "    + EX: \"A fluffy **dog** is a great pet\" <--> \"A fluffy **cat** is a great pet\"\n",
    "\n",
    "- By training a text-generation model, we wind up with a lookup table where each word has its own vector "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-using-word2vec-online-ds-ft-100719/master/images/new_skip_gram_net_arch.png\">\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-students/dsc-using-word2vec-online-ds-ft-100719/master/images/new_word2vec_weight_matrix_lookup_table.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word will have a vector of contexts: the embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Word Embeddings with Trump's Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T16:24:15.058841Z",
     "start_time": "2020-02-18T16:24:09.458946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fsds_1007219  v0.7.6 loaded.  Read the docs: https://fsds.readthedocs.io/en/latest/ \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_1a0f7918_526b_11ea_a247_acde48001122\" ><caption>Loaded Packages and Handles</caption><thead>    <tr>        <th class=\"col_heading level0 col0\" >Handle</th>        <th class=\"col_heading level0 col1\" >Package</th>        <th class=\"col_heading level0 col2\" >Description</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_1a0f7918_526b_11ea_a247_acde48001122row0_col0\" class=\"data row0 col0\" >dp</td>\n",
       "                        <td id=\"T_1a0f7918_526b_11ea_a247_acde48001122row0_col1\" class=\"data row0 col1\" >IPython.display</td>\n",
       "                        <td id=\"T_1a0f7918_526b_11ea_a247_acde48001122row0_col2\" class=\"data row0 col2\" >Display modules with helpful display and clearing commands.</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_1a0f7918_526b_11ea_a247_acde48001122row1_col0\" class=\"data row1 col0\" >fs</td>\n",
       "                        <td id=\"T_1a0f7918_526b_11ea_a247_acde48001122row1_col1\" class=\"data row1 col1\" >fsds_100719</td>\n",
       "                        <td id=\"T_1a0f7918_526b_11ea_a247_acde48001122row1_col2\" class=\"data row1 col2\" >Custom data science bootcamp student package</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_1a0f7918_526b_11ea_a247_acde48001122row2_col0\" class=\"data row2 col0\" >mpl</td>\n",
       "                        <td id=\"T_1a0f7918_526b_11ea_a247_acde48001122row2_col1\" class=\"data row2 col1\" >matplotlib</td>\n",
       "                        <td id=\"T_1a0f7918_526b_11ea_a247_acde48001122row2_col2\" class=\"data row2 col2\" >Matplotlib's base OOP module with formatting artists</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_1a0f7918_526b_11ea_a247_acde48001122row3_col0\" class=\"data row3 col0\" >plt</td>\n",
       "                        <td id=\"T_1a0f7918_526b_11ea_a247_acde48001122row3_col1\" class=\"data row3 col1\" >matplotlib.pyplot</td>\n",
       "                        <td id=\"T_1a0f7918_526b_11ea_a247_acde48001122row3_col2\" class=\"data row3 col2\" >Matplotlib's matlab-like plotting module</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_1a0f7918_526b_11ea_a247_acde48001122row4_col0\" class=\"data row4 col0\" >np</td>\n",
       "                        <td id=\"T_1a0f7918_526b_11ea_a247_acde48001122row4_col1\" class=\"data row4 col1\" >numpy</td>\n",
       "                        <td id=\"T_1a0f7918_526b_11ea_a247_acde48001122row4_col2\" class=\"data row4 col2\" >scientific computing with Python</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_1a0f7918_526b_11ea_a247_acde48001122row5_col0\" class=\"data row5 col0\" >pd</td>\n",
       "                        <td id=\"T_1a0f7918_526b_11ea_a247_acde48001122row5_col1\" class=\"data row5 col1\" >pandas</td>\n",
       "                        <td id=\"T_1a0f7918_526b_11ea_a247_acde48001122row5_col2\" class=\"data row5 col2\" >High performance data structures and tools</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_1a0f7918_526b_11ea_a247_acde48001122row6_col0\" class=\"data row6 col0\" >sns</td>\n",
       "                        <td id=\"T_1a0f7918_526b_11ea_a247_acde48001122row6_col1\" class=\"data row6 col1\" >seaborn</td>\n",
       "                        <td id=\"T_1a0f7918_526b_11ea_a247_acde48001122row6_col2\" class=\"data row6 col2\" >High-level data visualization library based on matplotlib</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1a238270f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[i] Pandas .iplot() method activated.']\n"
     ]
    }
   ],
   "source": [
    "!pip install -U fsds_100719\n",
    "from fsds_100719.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T16:24:16.906686Z",
     "start_time": "2020-02-18T16:24:15.060840Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>id_str</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2016-12-01 14:37:57</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>My thoughts and prayers are with those affecte...</td>\n",
       "      <td>12-01-2016 14:37:57</td>\n",
       "      <td>12077</td>\n",
       "      <td>65724</td>\n",
       "      <td>False</td>\n",
       "      <td>804333718999539712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-12-01 14:38:09</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>Getting ready to leave for the Great State of ...</td>\n",
       "      <td>12-01-2016 14:38:09</td>\n",
       "      <td>9834</td>\n",
       "      <td>57249</td>\n",
       "      <td>False</td>\n",
       "      <td>804333771021570048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-12-01 22:52:10</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Heading to U.S. Bank Arena in Cincinnati Ohio ...</td>\n",
       "      <td>12-01-2016 22:52:10</td>\n",
       "      <td>5564</td>\n",
       "      <td>31256</td>\n",
       "      <td>False</td>\n",
       "      <td>804458095569158144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-12-02 02:45:18</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Thank you Ohio! Together we made history – and...</td>\n",
       "      <td>12-02-2016 02:45:18</td>\n",
       "      <td>17283</td>\n",
       "      <td>72196</td>\n",
       "      <td>False</td>\n",
       "      <td>804516764562374656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-12-03 00:44:20</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>The President of Taiwan CALLED ME today to wis...</td>\n",
       "      <td>12-03-2016 00:44:20</td>\n",
       "      <td>24700</td>\n",
       "      <td>111106</td>\n",
       "      <td>False</td>\n",
       "      <td>804848711599882240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-01 01:17:43</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @SenJohnKennedy: I think Speaker Pelosi is ...</td>\n",
       "      <td>01-01-2020 01:17:43</td>\n",
       "      <td>8893</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1212181071988703232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-01 01:18:47</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @DanScavino: https://t.co/CJRPySkF1Z</td>\n",
       "      <td>01-01-2020 01:18:47</td>\n",
       "      <td>10796</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1212181341078458369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-01 01:22:28</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Our fantastic First Lady! https://t.co/6iswto4WDI</td>\n",
       "      <td>01-01-2020 01:22:28</td>\n",
       "      <td>27567</td>\n",
       "      <td>132633</td>\n",
       "      <td>False</td>\n",
       "      <td>1212182267113680896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-01 01:30:35</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>HAPPY NEW YEAR!</td>\n",
       "      <td>01-01-2020 01:30:35</td>\n",
       "      <td>85409</td>\n",
       "      <td>576045</td>\n",
       "      <td>False</td>\n",
       "      <td>1212184310389850119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-01 03:12:07</td>\n",
       "      <td>Twitter Media Studio</td>\n",
       "      <td>https://t.co/EVAEYD1AgV</td>\n",
       "      <td>01-01-2020 03:12:07</td>\n",
       "      <td>25016</td>\n",
       "      <td>108830</td>\n",
       "      <td>False</td>\n",
       "      <td>1212209862094012416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14066 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   source  \\\n",
       "datetime                                    \n",
       "2016-12-01 14:37:57    Twitter for iPhone   \n",
       "2016-12-01 14:38:09   Twitter for Android   \n",
       "2016-12-01 22:52:10    Twitter for iPhone   \n",
       "2016-12-02 02:45:18    Twitter for iPhone   \n",
       "2016-12-03 00:44:20   Twitter for Android   \n",
       "...                                   ...   \n",
       "2020-01-01 01:17:43    Twitter for iPhone   \n",
       "2020-01-01 01:18:47    Twitter for iPhone   \n",
       "2020-01-01 01:22:28    Twitter for iPhone   \n",
       "2020-01-01 01:30:35    Twitter for iPhone   \n",
       "2020-01-01 03:12:07  Twitter Media Studio   \n",
       "\n",
       "                                                                  text  \\\n",
       "datetime                                                                 \n",
       "2016-12-01 14:37:57  My thoughts and prayers are with those affecte...   \n",
       "2016-12-01 14:38:09  Getting ready to leave for the Great State of ...   \n",
       "2016-12-01 22:52:10  Heading to U.S. Bank Arena in Cincinnati Ohio ...   \n",
       "2016-12-02 02:45:18  Thank you Ohio! Together we made history – and...   \n",
       "2016-12-03 00:44:20  The President of Taiwan CALLED ME today to wis...   \n",
       "...                                                                ...   \n",
       "2020-01-01 01:17:43  RT @SenJohnKennedy: I think Speaker Pelosi is ...   \n",
       "2020-01-01 01:18:47            RT @DanScavino: https://t.co/CJRPySkF1Z   \n",
       "2020-01-01 01:22:28  Our fantastic First Lady! https://t.co/6iswto4WDI   \n",
       "2020-01-01 01:30:35                                    HAPPY NEW YEAR!   \n",
       "2020-01-01 03:12:07                            https://t.co/EVAEYD1AgV   \n",
       "\n",
       "                              created_at  retweet_count  favorite_count  \\\n",
       "datetime                                                                  \n",
       "2016-12-01 14:37:57  12-01-2016 14:37:57          12077           65724   \n",
       "2016-12-01 14:38:09  12-01-2016 14:38:09           9834           57249   \n",
       "2016-12-01 22:52:10  12-01-2016 22:52:10           5564           31256   \n",
       "2016-12-02 02:45:18  12-02-2016 02:45:18          17283           72196   \n",
       "2016-12-03 00:44:20  12-03-2016 00:44:20          24700          111106   \n",
       "...                                  ...            ...             ...   \n",
       "2020-01-01 01:17:43  01-01-2020 01:17:43           8893               0   \n",
       "2020-01-01 01:18:47  01-01-2020 01:18:47          10796               0   \n",
       "2020-01-01 01:22:28  01-01-2020 01:22:28          27567          132633   \n",
       "2020-01-01 01:30:35  01-01-2020 01:30:35          85409          576045   \n",
       "2020-01-01 03:12:07  01-01-2020 03:12:07          25016          108830   \n",
       "\n",
       "                    is_retweet               id_str  \n",
       "datetime                                             \n",
       "2016-12-01 14:37:57      False   804333718999539712  \n",
       "2016-12-01 14:38:09      False   804333771021570048  \n",
       "2016-12-01 22:52:10      False   804458095569158144  \n",
       "2016-12-02 02:45:18      False   804516764562374656  \n",
       "2016-12-03 00:44:20      False   804848711599882240  \n",
       "...                        ...                  ...  \n",
       "2020-01-01 01:17:43       True  1212181071988703232  \n",
       "2020-01-01 01:18:47       True  1212181341078458369  \n",
       "2020-01-01 01:22:28      False  1212182267113680896  \n",
       "2020-01-01 01:30:35      False  1212184310389850119  \n",
       "2020-01-01 03:12:07      False  1212209862094012416  \n",
       "\n",
       "[14066 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/jirvingphd/capstone-project-using-trumps-tweets-to-predict-stock-market/master/data/trump_tweets_12012016_to_01012020.csv')\n",
    "df['datetime'] = pd.to_datetime(df['created_at'])\n",
    "df = df.set_index('datetime').sort_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "\n",
    "- Two Part Word2Vec Tutorial  (linked from Learn)\n",
    "    - [Part 1: The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "    - [Part 2: Negative Sampling](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `sentences`: dataset to train on\n",
    "- `size`: how big of a word vector do we want\n",
    "- `window`: how many words around the target word to train with\n",
    "- `min_count`: how many times the word shows up in corpus; we don't want words that are rarely used\n",
    "- `workers`: number of threads (individual task \"workers\")\n",
    "\n",
    "```python\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Let's assume we have our text corpus already tokenized and stored inside the variable 'data'--the regular text preprocessing steps still need to be handled before training a Word2Vec model!\n",
    "\n",
    "model = Word2Vec(data, size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "model.train(data, total_examples=model.corpus_count)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T23:05:59.881105Z",
     "start_time": "2020-02-17T23:05:56.202912Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "## TRAINING WORD2VEC FROM FULL DF NOT JUST TARGETS\n",
    "data = df['text'].map(word_tokenize)\n",
    "data_lower = list(map(lambda x: [w.lower() for w in x],data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:24:37.208462Z",
     "start_time": "2020-02-17T17:24:37.203993Z"
    }
   },
   "outputs": [],
   "source": [
    "data[2],data_lower[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:24:38.695319Z",
     "start_time": "2020-02-17T17:24:37.210612Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(data_lower,size=100,window=4,min_count=1,workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:24:40.100326Z",
     "start_time": "2020-02-17T17:24:38.696892Z"
    }
   },
   "outputs": [],
   "source": [
    "model.train(data_lower, total_examples=model.corpus_count,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:24:40.103875Z",
     "start_time": "2020-02-17T17:24:40.101837Z"
    }
   },
   "outputs": [],
   "source": [
    "wv = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:24:40.123292Z",
     "start_time": "2020-02-17T17:24:40.105241Z"
    }
   },
   "outputs": [],
   "source": [
    "wv.most_similar('republican')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:24:40.128987Z",
     "start_time": "2020-02-17T17:24:40.124642Z"
    }
   },
   "outputs": [],
   "source": [
    "wv.most_similar(negative=['democrat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "## For initializing model\n",
    "sentences=None,\n",
    "    size=100,\n",
    "    alpha=0.025,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    max_vocab_size=None,\n",
    "    sample=0.001,\n",
    "    seed=1,\n",
    "    workers=3,\n",
    "    min_alpha=0.0001,\n",
    "    sg=0,\n",
    "    hs=0,\n",
    "    negative=5,\n",
    "    cbow_mean=1,\n",
    "    hashfxn=<built-in function hash>,\n",
    "    iter=5,\n",
    "    null_word=0,\n",
    "    trim_rule=None,\n",
    "    sorted_vocab=1,\n",
    "    batch_words=10000,\n",
    "    compute_loss=False,\n",
    "    callbacks=(),\n",
    "    \n",
    "    \n",
    "## For training \n",
    "    sentences,\n",
    "    total_examples=None,\n",
    "    total_words=None,\n",
    "    epochs=None,\n",
    "    start_alpha=None,\n",
    "    end_alpha=None,\n",
    "    word_count=0,\n",
    "    queue_factor=2,\n",
    "    report_delay=1.0,\n",
    "    compute_loss=False,\n",
    "    callbacks=(),\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:24:40.135065Z",
     "start_time": "2020-02-17T17:24:40.130341Z"
    }
   },
   "outputs": [],
   "source": [
    "# ### USING WORD VECTOR MATH TO GET A FEEL FOR QUALITY OF MODE\n",
    "def word_math(wv,pos_words=['hillary'],neg_words=['bill'],\n",
    "              verbose=True,return_vec=False):\n",
    "    if isinstance(pos_words,str):\n",
    "        pos_words=[pos_words]\n",
    "    if isinstance(neg_words,str):\n",
    "        neg_words=[neg_words]\n",
    "\n",
    "\n",
    "    pos_eqn = '+'.join(pos_words)\n",
    "    neg_eqn = '-'.join(neg_words)\n",
    "\n",
    "    print('---'*15)    \n",
    "    print(f\"[i] Result for:\\t{pos_eqn}{' - '+neg_eqn if len(neg_eqn)>0 else ' '}\")\n",
    "    print('---'*15)\n",
    "\n",
    "    answer = wv.most_similar(positive=pos_words,negative=neg_words)\n",
    "    \n",
    "    if verbose:\n",
    "          [print(f\"- {ans[0]} ({round(ans[1],3)})\") for ans in answer]\n",
    "          print('---'*15,'\\n\\n')\n",
    "\n",
    "    if return_vec: \n",
    "          return answer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:24:40.149995Z",
     "start_time": "2020-02-17T17:24:40.136523Z"
    }
   },
   "outputs": [],
   "source": [
    "equation_list=[(['america','crime'],[]),\n",
    "               \n",
    "               (['democrats','russia'],[]),\n",
    "               (['republican'],['honor']),\n",
    "               (['man','power'],[]),\n",
    "               (['russia','honor'],[]),\n",
    "              (['china','tariff'])]\n",
    "\n",
    "for eqn in equation_list:\n",
    "#     print('\\n\\n')\n",
    "    word_math(wv,*eqn)\n",
    "#     word_math(wv2,*eqn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# STOPPING POINT - SG Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## GloVe - Global Vectors for Word Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Usually embeddings are hundreds of dimensions\n",
    "- Just use the word embeddings already learned from before!\n",
    "    + Unless very specific terminology, context will likely carry within language\n",
    "- Comparable to CNN transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Visualize with t-SNE (t-Distributed Stochastic Neighbor Embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T22:57:37.693707Z",
     "start_time": "2020-02-17T22:57:37.685873Z"
    },
    "hidden": true
   },
   "source": [
    "Dimensionality reduction (like PCA)\n",
    "\n",
    "Tries to maintain relative distances (also works for images as well as words)\n",
    "\n",
    "Can identify relationships and bugs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Example of t-SNE visualization:** https://www.kaggle.com/colinmorris/visualizing-embeddings-with-t-sne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Using Embeddings in Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Embedding Layers\n",
    "You should make note of a couple caveats that come with using embedding layers in your neural network -- namely:\n",
    "\n",
    "* The embedding layer must always be the first layer of the network, meaning that it should immediately follow the `Input()` layer \n",
    "* All words in the text should be integer-encoded, with each unique word encoded as it's own unique integer  \n",
    "* The size of the embedding layer must always be greater than the total vocabulary size of the dataset! The first parameter denotes the vocabulary size, while the second denotes the size of the actual word vectors\n",
    "* The size of the sequences passed in as data must be set when creating the layer (all data will be converted to padded sequences of the same size during the preprocessing step) \n",
    "\n",
    "\n",
    "[Keras Documentation for Embedding Layers](https://keras.io/layers/embeddings/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Using Pre-Trained Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:24:40.155714Z",
     "start_time": "2020-02-17T17:24:40.151377Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "folder = '/Users/jamesirving/Datasets/'#glove.twitter.27B/'\n",
    "print(os.listdir(folder))\n",
    "glove_file = folder+'glove.6B/glove.6B.50d.txt'#'glove.twitter.27B.50d.txt'\n",
    "glove_twitter_file = folder+'glove.twitter.27B/glove.twitter.27B.50d.txt'\n",
    "print(glove_file)\n",
    "print(glove_twitter_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Keeping only the vectors needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:24:40.200170Z",
     "start_time": "2020-02-17T17:24:40.156915Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## This line of code for getting all words bugs me\n",
    "total_vocabulary = set(word for tweet in data_lower for word in tweet)\n",
    "len(total_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:24:41.358580Z",
     "start_time": "2020-02-17T17:24:40.201511Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "glove = {}\n",
    "with open(glove_file,'rb') as f:#'glove.6B.50d.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in total_vocabulary:\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:24:41.365627Z",
     "start_time": "2020-02-17T17:24:41.359883Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "glove['republican']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Using Embeddings in Classification Models - sci-kit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Embeddings can be used in Artificial Neural Networks as an input Embedding Layer\n",
    "- Embeddings can be used in sci-kit learn models by taking the mean vector of a text/document and using the mean vector as the input into the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Make Finding Trump Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- At the beginning of his presidency, Trump continued to use a non-secure Android phone for his personal use.\n",
    "- Tweets from this period can be attributed to Trump or his staffers based on if it came from an Android device or an iPhone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/jirvingphd/capstone-project-using-trumps-tweets-to-predict-stock-market/master/data/trump_tweets_12012016_to_01012020.csv')\n",
    "df['datetime'] = pd.to_datetime(df['created_at'])\n",
    "df = df.set_index('datetime').sort_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T23:07:32.914362Z",
     "start_time": "2020-02-17T23:07:32.905463Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "devices = ['Twitter for Android','Twitter for iPhone']\n",
    "print(f'The first and last timestamps for the group {devices[0]}are:')\n",
    "start,end= df.groupby('source').get_group(devices[0]).index[[0,-1]]\n",
    "print(start) ,print(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T23:20:59.695893Z",
     "start_time": "2020-02-17T23:20:59.677168Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Puttig it all together\n",
    "df_data = df[df['source'].isin(devices)].loc[start:end].copy()\n",
    "df_data['source'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T23:21:00.505149Z",
     "start_time": "2020-02-17T23:21:00.486213Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "undersample = False\n",
    "\n",
    "if undersample:\n",
    "    ## Undersampling to match class\n",
    "    iphone = df_data.loc[df_data['source']=='Twitter for iPhone']\n",
    "    android = df_data.loc[df_data['source']=='Twitter for Android']\n",
    "    print(len(iphone),len(android))\n",
    "    df = pd.concat([iphone, \n",
    "                    android.sample(n=len(iphone),\n",
    "                                   random_state=123)],\n",
    "                    axis=0)\n",
    "\n",
    "else:\n",
    "    df= df_data\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T23:21:01.168716Z",
     "start_time": "2020-02-17T23:21:01.166651Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # df.to_csv('datasets/trump_tweets_iphone_vs_twitter.csv',index=False)\n",
    "# df= pd.read_csv('datasets/trump_tweets_iphone_vs_twitter.csv')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T23:21:01.870605Z",
     "start_time": "2020-02-17T23:21:01.865285Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['source'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Creating Mean Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T23:21:03.776158Z",
     "start_time": "2020-02-17T23:21:03.609500Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize\n",
    "\n",
    "y = pd.get_dummies(df['source'],drop_first=True).values\n",
    "X = df['text'].str.lower().map(word_tokenize)\n",
    "\n",
    "X_idx = list(range(len(X)))\n",
    "train_idx,test_idx = train_test_split(X_idx,random_state=123)\n",
    "\n",
    "X[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T23:21:05.121322Z",
     "start_time": "2020-02-17T23:21:05.116054Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train_test_split_idx(X, y, train_idx,test_idx):\n",
    "    # try count vectorized first\n",
    "    X_train = X[train_idx].copy()\n",
    "    y_train = y[train_idx].copy()\n",
    "    X_test = X[train_idx].copy()\n",
    "    y_test = y[train_idx].copy()\n",
    "    return X_train, X_test,y_train, y_test\n",
    "\n",
    "X_train, X_test,y_train, y_test = train_test_split_idx(X,y,train_idx,test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T23:21:05.870815Z",
     "start_time": "2020-02-17T23:21:05.868769Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df['combined_text'] = df['headline'] + ' ' + df['short_description']\n",
    "# data = df['combined_text'].map(word_tokenize).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T23:21:06.540160Z",
     "start_time": "2020-02-17T23:21:06.534567Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class W2vVectorizer(object):\n",
    "    \n",
    "    def __init__(self, w2v):\n",
    "        # Takes in a dictionary of words and vectors as input\n",
    "        self.w2v = w2v\n",
    "        if len(w2v) == 0:\n",
    "            self.dimensions = 0\n",
    "        else:\n",
    "            self.dimensions = len(w2v[next(iter(glove))])\n",
    "    \n",
    "    # Note: Even though it doesn't do anything, it's required that this object implement a fit method or else\n",
    "    # it can't be used in a scikit-learn pipeline  \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                   or [np.zeros(self.dimensions)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:24:41.589103Z",
     "start_time": "2020-02-17T17:24:41.587325Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# target = df['source']\n",
    "# data = df['text'].map(word_tokenize)\n",
    "# data_lower = list(map(lambda x: [w.lower() for w in x],data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T23:21:11.415656Z",
     "start_time": "2020-02-17T23:21:11.066487Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf =  Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "              ('Random Forest', RandomForestClassifier(n_estimators=100, verbose=True))])\n",
    "svc = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "                ('Support Vector Machine', SVC())])\n",
    "lr = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "              ('Logistic Regression', LogisticRegression())])\n",
    "\n",
    "models = [('Random Forest', rf),\n",
    "          ('Support Vector Machine', svc),\n",
    "          ('Logistic Regression', lr)]\n",
    "# models = {'Random Forest':RandomForestClassifier(n_estimators=100, verbose=True),\n",
    "#           'SVC':SVC(),'lr':LogisticRegression()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T23:21:11.598068Z",
     "start_time": "2020-02-17T23:21:11.586333Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scores = [(name, cross_val_score(model, X_train, y_train, cv=2).mean()) for name, model, in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:24:42.345142Z",
     "start_time": "2020-02-17T17:24:42.340398Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Using Embedding Layers in ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:24:43.753059Z",
     "start_time": "2020-02-17T17:24:42.347211Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:24:43.758659Z",
     "start_time": "2020-02-17T17:24:43.754157Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from py_files import keras_gridsearch as kg\n",
    "from sklearn import metrics\n",
    "from fsds_100719.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:24:43.952550Z",
     "start_time": "2020-02-17T17:24:43.759802Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import text,sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "y_t = pd.get_dummies(df['source']).values\n",
    "X = df['text'].str.lower().map(word_tokenize)\n",
    "\n",
    "\n",
    "MAX_WORDS = 25000\n",
    "tokenizer = text.Tokenizer(num_words=MAX_WORDS)\n",
    "\n",
    "tokenizer.fit_on_texts(X) #df['text'])\n",
    "sequences = tokenizer.texts_to_sequences(X) #df['text'])\n",
    "\n",
    "X_t = sequence.pad_sequences(sequences, maxlen=100)\n",
    "X_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:24:43.960868Z",
     "start_time": "2020-02-17T17:24:43.954646Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =train_test_split(X_t,y_t,random_state=123) \n",
    "X_train.shape,y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:24:51.283812Z",
     "start_time": "2020-02-17T17:24:43.963328Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 128 #where codealong get this?\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(MAX_WORDS, EMBEDDING_SIZE))\n",
    "model.add(LSTM(25,return_sequences=True))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',#'categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.1)\n",
    "\n",
    "y_hat_test = model.predict_classes(X_test)\n",
    "kg.evaluate_model(y_test,y_hat_test,history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T00:18:01.708621Z",
     "start_time": "2020-02-17T00:18:00.772373Z"
    },
    "hidden": true
   },
   "source": [
    "## RNN or GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:24:51.890122Z",
     "start_time": "2020-02-17T17:24:51.285319Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## GRU Model\n",
    "from keras import models, layers, optimizers, regularizers\n",
    "modelG = models.Sequential()\n",
    "\n",
    "## Get and add embedding_layer\n",
    "# embedding_layer = ji.make_keras_embedding_layer(wv, X_train)\n",
    "modelG.add(Embedding(MAX_WORDS, EMBEDDING_SIZE))\n",
    "\n",
    "# modelG.add(layers.SpatialDropout1D(0.5))\n",
    "# modelG.add(layers.Bidirectional(layers.GRU(units=100, dropout=0.5, recurrent_dropout=0.2,return_sequences=True)))\n",
    "modelG.add(layers.Bidirectional(layers.GRU(units=100, dropout=0.5, recurrent_dropout=0.2)))\n",
    "modelG.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "modelG.compile(loss='categorical_crossentropy',optimizer=\"adam\",metrics=['acc'])#,'val_acc'])#, callbacks=callbacks)\n",
    "modelG.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.401380Z",
     "start_time": "2020-02-17T17:24:51.891216Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "history = modelG.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "y_hat_test = modelG.predict_classes(X_test)\n",
    "kg.evaluate_model(y_test,y_hat_test,history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learn vectorization\n",
    "WIP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.405690Z",
     "start_time": "2020-02-17T17:25:09.402826Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer,TfidfVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "tf_transformer = TfidfTransformer(use_idf=False)#TfidfTransformer()\n",
    "tfidf_transformer = TfidfTransformer(use_idf=True)#TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.411057Z",
     "start_time": "2020-02-17T17:25:09.407102Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le= LabelEncoder()\n",
    "y = le.fit_transform(df['source'])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.430325Z",
     "start_time": "2020-02-17T17:25:09.412237Z"
    }
   },
   "outputs": [],
   "source": [
    "X = count_vectorizer.fit_transform(df['text'])\n",
    "X_tf = tf_transformer.fit_transform(X)\n",
    "X_tfidf = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.435071Z",
     "start_time": "2020-02-17T17:25:09.431698Z"
    }
   },
   "outputs": [],
   "source": [
    "X_tf.shape,X_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.440786Z",
     "start_time": "2020-02-17T17:25:09.436593Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_idx = list(range(X.shape[0]))\n",
    "train_idx,test_idx = train_test_split(X_idx,random_state=123)\n",
    "\n",
    "\n",
    "def train_test_split_idx(X, y, train_idx,test_idx):\n",
    "    # try count vectorized first\n",
    "    X_train = X[train_idx].copy()\n",
    "    y_train = y[train_idx].copy()\n",
    "    X_test = X[train_idx].copy()\n",
    "    y_test = y[train_idx].copy()\n",
    "    return X_train, X_test,y_train, y_test\n",
    "\n",
    "X_dict = {'count':X_tf,\n",
    "         'tfidf':X_tfidf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.444364Z",
     "start_time": "2020-02-17T17:25:09.442174Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "\n",
    "# svc = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "#                 ('Support Vector Machine', SVC())])\n",
    "\n",
    "# lr = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "#               ('Logistic Regression', LogisticRegression())])\n",
    "\n",
    "# models = {'Random Forest':RandomForestClassifier(n_estimators=100, verbose=True),\n",
    "#           'SVC':SVC(),'lr':LogisticRegression()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.447730Z",
     "start_time": "2020-02-17T17:25:09.445744Z"
    }
   },
   "outputs": [],
   "source": [
    "# res = [['Method','Model',\"Result\"]]\n",
    "\n",
    "# for tf_type,X_data in X_dict.items():\n",
    "#     X_train, X_test,y_train, y_test = train_test_split_idx(X_data,y,train_idx,test_idx)\n",
    "    \n",
    "#     for name, model in models.items():\n",
    "    \n",
    "# #     rf = RandomForestClassifier(n_estimators=100,verbose=True)\n",
    "#         cv_res = cross_val_score(model, X_train,y_train, cv=5)\n",
    "#         res.append([tf_type,name,cv_res.mean()])\n",
    "\n",
    "# pd.DataFrame(res[1:],columns=res[0]).sort_values(\"Result\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.451259Z",
     "start_time": "2020-02-17T17:25:09.449187Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# rf_params = dict(n_estimators=100, verbose=True)\n",
    "\n",
    "# rf =Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "#               ('Random Forest',RandomForestClassifier(**rf_params))])\n",
    "\n",
    "# svc = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "#                 ('Support Vector Machine', SVC())])\n",
    "\n",
    "# lr = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "#               ('Logistic Regression', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.454810Z",
     "start_time": "2020-02-17T17:25:09.452823Z"
    }
   },
   "outputs": [],
   "source": [
    "# models = [('Random Forest', rf),\n",
    "#           ('Support Vector Machine', svc),\n",
    "#           ('Logistic Regression', lr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.458908Z",
     "start_time": "2020-02-17T17:25:09.456339Z"
    }
   },
   "outputs": [],
   "source": [
    "# # res = [['Model','Score']]\n",
    "# res=[['Model','Scores']]\n",
    "# for (name, model) in models:\n",
    "#     print(name)\n",
    "#     cv_res = cross_val_score(model, data_lower, df['source'], cv=5).mean()\n",
    "#     res.append([name,cv_res])\n",
    "    \n",
    "# pd.DataFrame(res[1:],columns=res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practicing Text Preprocessing with Trump's Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAST CLASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.463636Z",
     "start_time": "2020-02-17T17:25:09.460662Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make a list of stopwords to remove\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.472074Z",
     "start_time": "2020-02-17T17:25:09.465426Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get all the stop words in the English language\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list+=string.punctuation\n",
    "print(stopwords_list)\n",
    "stopwords_list.remove('until')\n",
    "stopwords_list.extend(['“','...','”'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.478267Z",
     "start_time": "2020-02-17T17:25:09.473964Z"
    }
   },
   "outputs": [],
   "source": [
    "## Commentary on not always accepting what is or isn't in stopwords\n",
    "'until' in stopwords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.854614Z",
     "start_time": "2020-02-17T17:25:09.480081Z"
    }
   },
   "outputs": [],
   "source": [
    "stopped_tokens = [w.lower() for w in tokens if w.lower() not in stopwords_list]\n",
    "freq = FreqDist(stopped_tokens)\n",
    "freq.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.856228Z",
     "start_time": "2020-02-17T17:24:26.416Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from ipywidgets import interact\n",
    "\n",
    "@interact\n",
    "def tokenize_tweet(i=(0,len(corpus)-1)):\n",
    "    from nltk.corpus import stopwords\n",
    "    import string\n",
    "    from nltk import word_tokenize,regexp_tokenize\n",
    "    \n",
    "    print(f\"- Tweet #{i}:\\n\")\n",
    "    print(corpus[i],'\\n')\n",
    "    tokens = word_tokenize(corpus[i])\n",
    "\n",
    "    # Get all the stop words in the English language\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    stopwords_list += string.punctuation\n",
    "    stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "    \n",
    "    print(tokens,end='\\n\\n')\n",
    "    print(stopped_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.857039Z",
     "start_time": "2020-02-17T17:24:26.417Z"
    }
   },
   "outputs": [],
   "source": [
    "## Get FreqDist for Cleaned Text Data\n",
    "corpus[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Phases of Proprocessing/Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.857820Z",
     "start_time": "2020-02-17T17:24:26.420Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def clean_text(text,exclude_words=['until']):\n",
    "#     from nltk.corpus import stopwords\n",
    "#     import string\n",
    "#     from nltk import word_tokenize,regexp_tokenize\n",
    "#     ## tokenize text\n",
    "#     tokens = word_tokenize(text)\n",
    "#     # Get all the stop words in the English language\n",
    "#     stopwords_list = stopwords.words('english')\n",
    "#     stopwords_list += string.punctuation\n",
    "#     stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "#     return stopped_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.858957Z",
     "start_time": "2020-02-17T17:24:26.421Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from ipywidgets import interact\n",
    "\n",
    "@interact\n",
    "def tokenize_tweet(i=(0,len(corpus)-1)):\n",
    "    from nltk.corpus import stopwords\n",
    "    import string\n",
    "    from nltk import word_tokenize,regexp_tokenize\n",
    "    \n",
    "    print(f\"- Tweet #{i}:\\n\")\n",
    "    print(corpus[i],'\\n')\n",
    "    tokens = word_tokenize(corpus[i])\n",
    "\n",
    "    # Get all the stop words in the English language\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    stopwords_list += string.punctuation\n",
    "    stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "    \n",
    "    print(tokens,end='\\n\\n')\n",
    "    print(stopped_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best regexp resource and tester: https://regex101.com/\n",
    "\n",
    "    - Make sure to check \"Python\" under Flavor menu on left side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.859790Z",
     "start_time": "2020-02-17T17:24:26.424Z"
    }
   },
   "outputs": [],
   "source": [
    "text =  corpus[6615]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.860630Z",
     "start_time": "2020-02-17T17:24:26.425Z"
    }
   },
   "outputs": [],
   "source": [
    "text2=corpus[7347]\n",
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.861635Z",
     "start_time": "2020-02-17T17:24:26.427Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import regexp_tokenize\n",
    "pattern = r\"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "regexp_tokenize(text,pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.862513Z",
     "start_time": "2020-02-17T17:24:26.428Z"
    }
   },
   "outputs": [],
   "source": [
    "print('[i] Word Tokenize:',end='\\n'+'---'*20+'\\n')\n",
    "print(word_tokenize(text))\n",
    "\n",
    "print('\\n[i] Regexp Tokenize:',end='\\n'+'---'*20+'\\n')\n",
    "print(regexp_tokenize(text,pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.863366Z",
     "start_time": "2020-02-17T17:24:26.431Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text,regex=True):\n",
    "    from nltk.corpus import stopwords\n",
    "    import string\n",
    "    from nltk import word_tokenize,regexp_tokenize\n",
    "\n",
    "    ## tokenize text\n",
    "    if regex:\n",
    "        pattern = r\"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "        tokens= regexp_tokenize(text,pattern)\n",
    "    else:\n",
    "        tokens = word_tokenize(text)\n",
    "    # Get all the stop words in the English language\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    stopwords_list += string.punctuation\n",
    "    stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "    return stopped_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.864322Z",
     "start_time": "2020-02-17T17:24:26.433Z"
    }
   },
   "outputs": [],
   "source": [
    "# @interact\n",
    "# def regexp_tokenize_tweet(i=(0,len(corpus)-1)):\n",
    "#     print(f\"- Tweet #{i}:\\n\")\n",
    "#     print(corpus[i],'\\n')\n",
    "#     from nltk import regexp_tokenize\n",
    "#     pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "#     tokens= regexp_tokenize(corpus[i],pattern)\n",
    "\n",
    "#     # It is usually a good idea to lowercase all tokens during this step, as well\n",
    "#     stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "#     print(tokens,end='\\n\\n')\n",
    "#     return print(stopped_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.865198Z",
     "start_time": "2020-02-17T17:24:26.434Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def find_urls(string): \n",
    "    return re.findall(r\"(http[s]?://\\w*\\.\\w*/+\\w+)\",string)\n",
    "\n",
    "def find_hashtags(string):\n",
    "    return re.findall(r'\\#\\w*',string)\n",
    "\n",
    "def find_retweets(string):\n",
    "    return re.findall(r'RT [@]?\\w*:',string)\n",
    "\n",
    "def find_mentions(string):\n",
    "    return re.findall(r'\\@\\w*',string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.866059Z",
     "start_time": "2020-02-17T17:24:26.436Z"
    }
   },
   "outputs": [],
   "source": [
    "find_urls(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.866930Z",
     "start_time": "2020-02-17T17:24:26.438Z"
    }
   },
   "outputs": [],
   "source": [
    "find_mentions(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming/Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.867831Z",
     "start_time": "2020-02-17T17:24:26.440Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize('feet')) # foot\n",
    "print(lemmatizer.lemmatize('running')) # run [?!] Does not match expected output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.868746Z",
     "start_time": "2020-02-17T17:24:26.442Z"
    }
   },
   "outputs": [],
   "source": [
    "text_in =  corpus[6615]\n",
    "\n",
    "# # urls = find_urls(text)\n",
    "# def clean_text(text,regex=True):\n",
    "#     from nltk.corpus import stopwords\n",
    "#     import string\n",
    "#     from nltk import word_tokenize,regexp_tokenize\n",
    "\n",
    "#     ## tokenize text\n",
    "#     if regex:\n",
    "#         pattern = r\"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "#         tokens= regexp_tokenize(text,pattern)\n",
    "#     else:\n",
    "#         tokens = word_tokenize(text)\n",
    "#     # Get all the stop words in the English language\n",
    "#     stopwords_list = stopwords.words('english')\n",
    "#     stopwords_list += string.punctuation\n",
    "#     stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "#     return stopped_tokens\n",
    "\n",
    "def process_tweet(text,as_lemmas=False,as_tokens=True):\n",
    "#     text=text.copy()\n",
    "    for x in find_urls(text):\n",
    "        text = text.replace(x,'')\n",
    "        \n",
    "    for x in find_retweets(text):\n",
    "        text = text.replace(x,'')    \n",
    "        \n",
    "    for x in find_hashtags(text):\n",
    "        text = text.replace(x,'')    \n",
    "\n",
    "    if as_lemmas:\n",
    "        from nltk.stem.wordnet import WordNetLemmatizer\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        text = lemmatizer.lemmatize(text)\n",
    "    \n",
    "    if as_tokens:\n",
    "        text = clean_text(text)\n",
    "    \n",
    "    if len(text)==0:\n",
    "        text=''\n",
    "            \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.869726Z",
     "start_time": "2020-02-17T17:24:26.444Z"
    }
   },
   "outputs": [],
   "source": [
    "@interact\n",
    "def show_processed_text(i=(0,len(corpus)-1)):\n",
    "    text_in = corpus[i]#.copy()\n",
    "    print(text_in)\n",
    "    text_out = process_tweet(text_in)\n",
    "    print(text_out)\n",
    "    text_out2 = process_tweet(text_in,as_lemmas=True)\n",
    "    print(text_out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.870632Z",
     "start_time": "2020-02-17T17:24:26.446Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Potential Tasks: Classify Android vs iPhone tweets (from period where Android tweets still exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.871534Z",
     "start_time": "2020-02-17T17:24:26.449Z"
    }
   },
   "outputs": [],
   "source": [
    "df['datetime'] = pd.to_datetime(df['created_at'])\n",
    "df\n",
    "\n",
    "df = df.set_index('datetime').sort_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.872515Z",
     "start_time": "2020-02-17T17:24:26.451Z"
    }
   },
   "outputs": [],
   "source": [
    "df['clean_text'] = df['text'].apply(process_tweet)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.873433Z",
     "start_time": "2020-02-17T17:24:26.454Z"
    }
   },
   "outputs": [],
   "source": [
    "android = df.groupby('source').get_group('Twitter for Android')\n",
    "android.index\n",
    "\n",
    "iphone = df.groupby('source').get_group('Twitter for iPhone').loc[:android.index[-1]]\n",
    "iphone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.874347Z",
     "start_time": "2020-02-17T17:24:26.456Z"
    }
   },
   "outputs": [],
   "source": [
    "len(android), len(iphone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.875334Z",
     "start_time": "2020-02-17T17:24:26.458Z"
    }
   },
   "outputs": [],
   "source": [
    "df_corpus = pd.concat([iphone,android],axis=0)\n",
    "df_corpus['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Count vectorization\n",
    "- Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "    -  Used for multiple texts\n",
    "    \n",
    "    \n",
    "**_Term Frequency_** is calculated with the following formula:\n",
    "\n",
    "$$ \\text{Term Frequency}(t) = \\frac{\\text{number of times it appears in a document}} {\\text{total number of terms in the document}} $$ \n",
    "\n",
    "**_Inverse Document Frequency_** is calculated with the following formula:\n",
    "\n",
    "$$ IDF(t) = log_e(\\frac{\\text{Total Number of Documents}}{\\text{Number of Documents with it in it}})$$\n",
    "\n",
    "The **_TF-IDF_** value for a given word in a given document is just found by multiplying the two!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions/Topics \n",
    "- Next time: vectorization\n",
    "- Vs Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.876256Z",
     "start_time": "2020-02-17T17:24:26.462Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:25:09.877148Z",
     "start_time": "2020-02-17T17:24:26.464Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer.fit_transform(df_corpus['clean_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "232.727px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
